---
title: "Ordinary Least Squares Theory"
subtitle: "Kevin's PSPE Notes: Econometrics/Causal Inference"
format:
    html:
        page-layout: article
        grid:
          sidebar-width: 350px
          body-width: 700px
          margin-width: 250px
          gutter-width: 2em
        fontsize: 12pt
        theme: default
        toc: TRUE
        toc-depth: 3
        toc-location: left
        toc-expand: true
        toc-title: "Table of Contents"
        mainfont: "Computer Modern"
        title-block-banner: true
        classoption: fleqn
        html-math-method: katex
editor: visual
---

Use the sidebar for quick navigation.

These supplementary notes are useful as OLS is frequently used in different causal estimation designs, so it is important to know in which situations OLS can be used effectively (in fact, many causal designs were created to solve issues with OLS).

This section is very mathematics heavy (a lot of summation algebra, probability, and some linear algebra).

------------------------------------------------------------------------

# **Ordinary Least Squares Estimator**

### Sum of Squared Residuals

To estimate the population parameters $\beta_0, \dots, \beta_k$, we use our sample data, and try to find the values $\widehat{\beta_0}, \dots, \widehat{\beta_k}$ that **minimise the square sum of residuals** (SSR):

$$
\begin{split}
SSR & = \sum\limits_{i=1}^n(y_i - \hat y_i)^2 \\
& = \sum\limits_{i=1}^n(y_i - \widehat{\beta_0} - \widehat{\beta_1}x_{1i} - \dots - \widehat{\beta_k}x_{ki})
\end{split}
$$

::: {.callout-tip collapse="true" appearance="simple"}
## Intuitive Visualisation of SSR

The residuals are the difference from our predicted best-fit line result $\widehat{y_i}$, and the actual value of $y_i$ in the data. Below highlighted in red are the residuals.

![](images/clipboard-846785636.png){fig-align="center" width="70%"}

After we have the residual values, we simply square each of them, then sum all of them together. That is the sum of squared residuals.
:::

This estimation is called the **ordinary least squares (OLS) estimator**. The solutions to the OLS estimator can be derived mathematically.

::: {.callout-tip collapse="true" appearance="simple"}
## Why Squared?

The residuals are squared because we care about the magnitude of errors, not the direction of error. For example, look at this figure:

![](images/clipboard-846785636.png){fig-align="center" width="70%"}

Here, the residuals d1, d3, and dn are positive, but the residuals of d2 and d4 are negative. If we just add them together, the negative and positive residuals would cancel out. But by squaring them, we are measuring the magnitude, not the direction of error.

Why not absolute value?

1.  Absolute value function is not differentiable at its vertex, which makes finding a closed-form mathematical solution much more difficult.
2.  Squaring the residuals punishes larger errors more. As we will explore later, this makes OLS a very desirable estimator that is unbiased and more efficient than an absolute value estimator.
:::

<br />

### Deriving the Estimator

For simple linear regression, the solutions of the estimator are:

$$
\begin{split}
\widehat{\beta_1} & = \frac{\sum_{i=1}^n (x_i - \bar x)(y_i - \bar y)}{\sum_{i=1}^n (x_i - \bar x)^2} = \frac{S.Cov(x,y)}{S.Var(x)} \\
& \\
\widehat{\beta_0} & = \bar y - \widehat{\beta_1}\bar x
\end{split}
$$

::: {.callout-tip collapse="true" appearance="simple"}
## Derivation of OLS for Simple Linear Regression

Let us define the sum of squared residuals as function $S$.

First, let us find the partial derivative of $S$ in respect to $\hat\beta_0$. We can do this using chain rule (for each individual part of the summation) and sum rule (for the summation).

$$
\begin{split}
\frac{\partial S(\hat{\beta}_0, \hat{\beta}_1)}{\partial \hat{\beta}_0} & = \frac{\partial }{\partial \hat{\beta}_0} \left[ \sum\limits_{i=1}^n (y_i - \hat{\beta}_0 - \hat{\beta}_1x_i)^2 \right] \\
& = -2 \sum\limits_{i=1}^n (y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i)
\end{split}
$$

To minimises $S$, we set the derivative equal to 0. We can ignore the -2, since if the summation is equal to 0, everything is 0. Thus, the first order condition is:

$$
\sum\limits_{i=1}^n (y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i) = 0
$$

Now, let us do the same for $\hat\beta_1$. Using the same steps as before

$$
\begin{split}
\frac{\partial S(\hat{\beta}_0, \hat{\beta}_1)}{\partial \hat{\beta}_1} & = \sum\limits_{i=1}^n \left[ -2x_i(y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i) \right] \\
& = -2 \sum\limits_{i=1}^n x_i(y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i)
\end{split}
$$

The first order condition for $\hat\beta_1$ will be:

$$
\sum\limits_{i=1}^n x_i(y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i) = 0
$$

<br />

We now have our two first-order conditions. Now, we have a 2-equation system of equations, with 2 variables. First, let us solve the first equation for $\hat\beta_0$ in terms of $\hat\beta_1$:

$$
\begin{split}
\sum\limits_{i=1}^n (y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i) & =  0 \\
\sum\limits_{i=1}^n y_i - n \hat{\beta}_0 - \hat{\beta}_1 \sum\limits_{i=1}^n x_i & = 0 \\
-n\hat{\beta}_0 &= -\sum\limits_{i=1}^n y_i + \hat{\beta}_1\sum\limits_{i=1}^nx_i \\
\hat\beta_0 & = \frac{1}{-n} \left( -\sum\limits_{i=1}^n y_i + \hat{\beta}_1\sum\limits_{i=1}^n x_i \right) \\
\hat{\beta}_0 & = \frac{1}{n} \sum\limits_{i=1}^n y_i - \frac{1}{n}\hat{\beta}_1 \sum\limits_{i=1}^n x_i \\
\hat\beta_0& = \bar{y} - \hat{\beta}_1 \bar{x}
\end{split}
$$

Now, let us substitute our calculated $\hat{\beta}_0 = \bar{y} - \hat{\beta}_1 \bar{x}$ into the $\hat{\beta}_1$ condition and solve for $\hat{\beta}_1$:

$$
\begin{split}
0 & =\sum\limits_{i=1}^n x_i (y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i)  \\
& = \sum\limits_{i=1}^n \left[ x_i(y_i - [\bar{y} - \hat{\beta}_1\bar{x}] - \hat{\beta}_1x_i) \right] \\
& = \sum\limits_{i=1}^n \left[ x_i(y_i - \bar{y} - \hat{\beta}_1 (x_i - \bar{x})) \right] \\
& = \sum\limits_{i=1}^n \left[ x_i(y_i - \bar{y}) - x_i \hat{\beta}_1(x_i - \bar{x}) \right] \\
& = \sum\limits_{i=1}^n x_i (y_i - \bar{y}) - \hat{\beta}_1 \sum\limits_{i=1}^nx_i (x_i - \bar{x})
\end{split}
$$

::: {.callout-warning collapse="true"}
## Useful Properties of Summation

Before we finish, here are a few key properties of summation

**Property 1:**

$$
\sum\limits_{i=1}^n (x_i - \bar{x}) = 0
$$

-   This is because we can expand the left to $\sum x_i - \sum \bar x$.
-   Then, we know $\sum x_i = \sum \bar x$ (by the formula for mean), so $\sum x_i - \sum \bar x = 0$.

**Property 2:**

$$
\sum\limits_{i=1}^n x_i(y_i - \bar{y}) = \sum\limits_{i=1}^n(x_i - \bar{x}) (y_i - \bar{y}) 
$$

-   This is because on the right side can expand to $\sum [x_i(y_i - \bar y) - \bar x (y_i - \bar y)]$.
-   Then, split into $\sum x_i (y_i - \bar y) - \bar x \sum (y_i - \bar y)$.
-   We know that by property 1 (which applies to any variable), $\sum (y_i - \bar y) = 0$. Thus, the right side disappears, and we are left with $\sum x_i (y_i - \bar y)$.

**Property 3:**

$$
\sum\limits_{i=1}^n x_i(x_i - \bar{x}) = \sum\limits_{i=1}^n(x_i - \bar{x})^2
$$

-   Start by expanding right side to $\sum [ x_i ( x_i - \bar x) - \bar x (x_i - \bar x)]$
-   Which splits into $\sum x_i (x_i - \bar x) - \bar x \sum (x_i - \bar x)$
-   By the first property, we know $\sum x_i - \bar x = 0$, so we are only left with $\sum x_i (x_i - \bar x)$
:::

Knowing properties of summation, we can transform what we had before:

$$
\begin{split}
0 & = \sum\limits_{i=1}^n x_i (y_i - \bar{y}) - \hat{\beta}_1 \sum\limits_{i=1}^nx_i (x_i - \bar{x}) \\
0 & = \sum\limits_{i=1}^n(x_i - \bar{x})(y_i - \bar{y}) - \hat{\beta}_1 \sum\limits_{i=1}^n (x_i - \bar{x})^2 \\
\hat{\beta}_1 \sum\limits_{i=1}^n (x_i - \bar{x})^2 & = \sum\limits_{i=1}^n(x_i - \bar{x})(y_i - \bar{y}) \\
\hat{\beta}_1 & = \frac{\sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^n(x_i - \bar{x})^2}
\end{split}
$$

Note that the numerator is equivalent to the formula of covariance $Cov(x,y)$, and the denominator is equal to the variance $Var(x)$.

We found that $\hat\beta_0 = \bar{y} - \hat{\beta}_1 \bar{x}$ earlier, so we just plug our solution of $\hat\beta_1$ in to get $\hat\beta_0$.
:::

For multiple linear regression, all coefficients are given by the vector $\hat\beta$:

$$
\hat\beta = (X'X)^{-1}X'y
$$

::: {.callout-tip collapse="true" appearance="simple"}
## Derivation of OLS for Multiple Linear Regression

Let us define our estimation vector $\hat{\beta}$ as the value of $\hat\beta$ that minimises the sum of squared errors:

$$
\hat{\beta} = \min\limits_{b} (y - Xb)' (y - Xb) = \min\limits_b S(b)
$$

-   $(y - Xb)$ is our error, since $\hat y = Xb$,

We can expand $S(b)$ as follows:

$$
\begin{split}
S(b) & = y'y - b'X'y - y'Xb + b'X'Xb \\ 
& = y'y - 2b'X'y + b'X'Xb
\end{split}
$$

Taking the partial derivative in respect to $b$:

$$
\frac{\partial S(b)}{\partial b} = \begin{pmatrix}\frac{\partial S(b)}{\partial b_1} \\\vdots \\\frac{\partial S(b)}{\partial b_k}\end{pmatrix}
$$

Differentiating with the vector $b$ yields:

$$
\frac{\partial S(b)}{\partial b} = -2X'y + 2X'Xb
$$

Evaluated at $\hat{\beta}$, the derivatives should equal zero (since first order condition of finding minimums):

$$
\frac{\partial S(b)}{\partial b} \biggr|_{\hat{\beta}} = -2X'y + 2X'X \hat{\beta} = 0
$$

When assuming $X'X$ is invertable, we can isolate $\hat{\beta}$ to find the solution to OLS:

$$
\begin{split}
-2X'y + 2X'X \hat{\beta} & = 0 \\
2X'X\hat\beta & = 2X'y \\
\hat\beta & = (2X'X)^{-1} 2 X'y \\
\hat\beta & = (X'X)^{-1}X'y
\end{split}
$$
:::

<br />

### Algebraic Properties of OLS

OLS Residuals $\hat u_i$ always sum to 0:

$$
\sum\limits_{i=1}^n \hat u_i = 0, \quad \text{where } \hat u_i = y_i - \widehat{y_i}
$$

::: {.callout-tip collapse="true" appearance="simple"}
## Proof OLS Residuals Sum to 0

Recall our OLS first order conditions:

$$
\begin{split}& \sum\limits_{i=1}^n (y_i - \hat{\beta}_0 - \hat{\beta}_1x_{1i} -  \hat{\beta}_2x_{2i}...) = 0 \\& \sum\limits_{i=1}^n x_{1i}(y_i - \hat{\beta}_0 - \hat{\beta}_1x_{1i} -  \hat{\beta}_2x_{2i}...) = 0 \\
& \sum\limits_{i=1}^n x_{2i} (y_i - \hat{\beta}_0 - \hat{\beta}_1x_{1i} -  \hat{\beta}_2x_{2i}...) = 0
\end{split}
$$

-   and so on for $x_{3i}, ..., x_{ki}$.

We know residuals $\hat u_i = y_i - \hat \beta_0 - \hat\beta_1x_{1i} - \dots - \hat\beta_k x_{ki}$. If we plug that definition of $\hat u_i$ into the first minimisation condition, we get:

$$
\sum\limits_{i=1}^n \hat u_i = 0
$$
:::

There is no covariance between any explanatory variable and the residuals.

$$
Cov(x_j, \hat u) = 0
$$

::: {.callout-tip collapse="true" appearance="simple"}
## Proof of no Covariance between Residuals and Regressors

Recall our OLS first order conditions:

$$
\begin{split}& \sum\limits_{i=1}^n (y_i - \hat{\beta}_0 - \hat{\beta}_1x_{1i} -  \hat{\beta}_2x_{2i}...) = 0 \\& \sum\limits_{i=1}^n x_{1i}(y_i - \hat{\beta}_0 - \hat{\beta}_1x_{1i} -  \hat{\beta}_2x_{2i}...) = 0 \\
& \sum\limits_{i=1}^n x_{2i} (y_i - \hat{\beta}_0 - \hat{\beta}_1x_{1i} -  \hat{\beta}_2x_{2i}...) = 0
\end{split}
$$

-   and so on for $x_{3i}, ..., x_{ki}$.

We know residuals $\hat u_i = y_i - \hat \beta_0 - \hat\beta_1x_{1i} - \dots - \hat\beta_k x_{ki}$.

-   Thus, plugging into the first condition, $\sum\limits_{i=1}^n \hat u_i = 0$.
-   Thus, plugging into the other conditions, $\sum\limits_{i=1}^n x_j \hat u_i = 0$.

For simplicity, we will use simple linear regression, but the same applies to multiple linear regression. Now, recall the formula for covariance:

$$
Cov(x,y) = \frac{1}{n}\sum\limits_{i=1}^n [(x_i - \bar x)(y_i - \bar y)] \\
$$

Thus, the covariance between $x_j$ (for notation simplicity, just $x$) and $\hat u$ is:

$$
\begin{split}
Cov(x, \hat u) & = \frac{1}{n}\sum\limits_{i=1}^n [(x_i - \bar x)(\hat u_i - \bar{\hat u})] \\
& = \frac{1}{n}\sum\limits_{i=1}^n(x_i \hat u_i - x_i \bar{\hat u} - \bar x \hat u_i + \bar x \bar {\hat u}) \\
& = \frac{1}{n} \left( \sum\limits_{i=1}^nx_i \hat u_i - \sum\limits_{i=1}^n x_i\bar{\hat u} - \sum\limits_{i=1}^n \bar x \hat u_i + \sum\limits_{i=1}^n\bar x \bar{\hat u} \right) \\
& = \frac{1}{n} \left( \sum\limits_{i=1}^nx_i \hat u_i - \bar{\hat u}\sum\limits_{i=1}^n x_i - \bar x\sum\limits_{i=1}^n \hat u_i + \bar{\hat u}\sum\limits_{i=1}^n\bar x  \right) \\
& = \frac{1}{n} \left( \sum\limits_{i=1}^nx_i \hat u_i - 0\sum\limits_{i=1}^n x_i - \bar x\sum\limits_{i=1}^n \hat u_i + 0\sum\limits_{i=1}^n\bar x  \right) \\
& = \frac{1}{n}(0 -0-\bar x(0) + 0) \\
& = 0
\end{split}
$$
:::

The OLS estimated coefficients produces a best-fit line, that always passes through the point $(\bar x_1, \dots, \bar x_k, \bar y)$, which is the point of the means of all the variables.

::: {.callout-tip collapse="true" appearance="simple"}
## Proof Fitted-Values Passes through Means

For simplicity, take simple linear regression. Remember our solution for $\hat\beta_0$ in OLS for simple linear regression was $\hat\beta_0 = \bar{y} - \hat{\beta}_1 \bar{x}$. Rearranging this equation, we get:

$$
\begin{split}
\hat\beta_0  & = \bar{y} - \hat{\beta}_1 \bar{x} \\
\hat\beta_0 + \hat\beta_1 \bar x & = \bar y \\
\bar y & = \hat\beta_0 + \hat\beta_1 \bar x
\end{split}
$$

Thus, the OLS estimated best-fit line always passes though point $(\bar x, \bar y)$ (the means of our data). The same property applies to multiple linear regression, but for point $(\bar x_1, \dots, \bar x_k, \bar y)$.
:::

<br />

### Regression Anatomy Theorem

The **Regression Anatomy Theorem**, also called the **Frisch-Waugh-Lovell** theorem, shows how multiple linear regression and OLS can be used to "control" for confounding variables.

Essentially, the theorem states that given explanatory variables $x_j= x_1, \dots, x_k$ and an outcome variable $y$, the coefficient $\beta_j$ (of any explanatory variable $x_j$) is the effect of the uncorrelated part of $x_j$ with all other explanatory variables, $\widetilde{r_j}$, on $y$.

This means by including control variables in our regression, our $\beta_j$ coefficient will partial out the effect of the controls, and only find the independent effect of $x_j$ on $y$.

::: {.callout-tip collapse="true" appearance="simple"}
## Proof of Regression Anatomy

Take our standard multiple linear regression:

$$
y_i = \beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i} + ... + \beta_k x_{ki} + u_i
$$

Let us say we are interested in $x_1$ (this can be generalised to any explanatory variable $x_j$). Let us make $x_1$ the outcome variable of a regression with explanatory variables $x_2, ..., x_k$:

$$
x_{1i} = \gamma_0 + \gamma_1 x_{2i} + ... + \gamma_{k-1}x_{ki} + \widetilde{r_{1i}}
$$

The error term is $\widetilde{r_{1i}}$, which represents the part of $x_{1i}$ that are uncorrelated to $x_2, ..., x_k$. In other words, $\widetilde{r_{1i}}$ is the part of $x_1$ that cannot be explained by any other explanatory variable $x_2, ..., x_k$. (uncorrelated with them)

<br />

Now, take the regression of with outcome variable $y$, with all explanatory variables [except]{.underline} $x_1$:

$$
y_i = \delta_0 + \delta_1 x_{2i} + ... + \delta_{k-1} x_{ki} + \widetilde{y_i}
$$

The error term is $\widetilde{y_i}$, which is the part of $y_i$ that cannot be explained by $x_2, ..., x_k$ (uncorrelated with them).

Since $\widetilde{y_i}$ is not explained by $x_2, ..., x_k$, variable $x_1$ must be the one explaining $\widetilde{y_i}$.

-   But, it is not the whole of $x_1$ explaining $\tilde{y_i}$. This is since $x_1$ may also correlated with $x_2, ..., x_k$, and the correlated parts of $x_1$ with $x_2, ..., x_k$ are already picked up in the regression by the coefficients of $x_2, ..., x_k$.

Thus, $\widetilde{y_i}$ must be explained by the part of $x_1$ that is uncorrelated and not explained by $x_2, ..., x_k$, which we derived earlier as $\widetilde{r_{1i}}$.

<br />

Thus, we can create another regression with explanatory variable $\widetilde{x_{1i}}$ and outcome variable $\widetilde{y_i}$.

$$
\widetilde{y_i} = \alpha_0 + \alpha_1 \widetilde{r_{1i}} + u_i
$$

We can plug $\widetilde{y_i}$ back into our regression of $y_i$ with explanatory variables $x_2 ..., x_k$, and re-arrange:

$$
\begin{split}
y_i & = \delta_0 + \delta_1 x_{2i} + ... + \delta_{k-1} x_{ki} + \widetilde{y_i} \\
y_i & = \delta_0 + \delta_1 x_{2i} + ... + \delta_{k-1} x_{ki} + \alpha_0 + \alpha_1 \widetilde{r_{1i}} + u_i \\
y_i  & = (\delta_0 + \alpha_0) + \alpha_1 \widetilde{r_{1i}} + \delta_1x_{2i} + ... + \delta_{k-1} x_{ki} + u_i 
\end{split}
$$

As we can see, this new regression mirrors the original standard multiple linear regression:

$$
\begin{split}
y_i  & = (\delta_0 + \alpha_0) + \alpha_1 \widetilde{r_{1i}} + \delta_1x_{2i} + ... + \delta_{k-1} x_{ki} + u_i \\
y_i & = \beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i} + \dots + \beta_k x_{ki} + u_i
\end{split}
$$

-   The $\beta_0$ in the original is analogous to the $(\delta_0 + \alpha 0)$.
-   The $\beta_1 x_{1i}$ in the original is analogous to $\alpha_1 \widetilde{r_{1i}}$.
-   The $\beta_2 x_{2i} + \dots + \beta_k x_{ki}$ is analogous to $\delta_1 x_{2i} + ... + \delta_{k-1} x_{ki}$.
-   The $u_i$ is in both regressions.

<br />

Importantly we know the $\beta_1 x_{1i}$ in the original is analogous to $\alpha_1 \widetilde{r_{1i}}$. Thus, [the estimate of $\alpha_1$ will be the same as $\beta_1$ in the original regression]{.underline}.

-   The coefficient $\alpha_1$ (which is equal to $\beta_1$) explains the expected change in $y$, given an increase in the part of $x_1$ uncorrelated with $x_2, ..., x_k$.
-   So essentially, [we have **partialed out** the effect of the other explanatory variables, and only focus on the effect on $y$ of the uncorrelated part of $x_1$ (which is $\widetilde{r_{1i}}$)]{.underline}
:::

This theorem allows us to essentially "rewrite" the solution of $\widehat{\beta_j}$ in multiple regression as a simple linear regression between $\widetilde{r_j}$ and $y$:

$$
\widehat{\beta_j} = \frac{\sum_{i=1}^n \widetilde{r_{ji}} y_i}{\sum_{i=1}^n(\widetilde{r_{ji}})^2}
$$

This formula is called the Regression Anatomy Formula.

::: {.callout-tip collapse="true" appearance="simple"}
## Deriving the Regression Anatomy Formula

::: {.callout-warning collapse="true"}
## Useful Properties of Summation

Before we start, here are a few key properties of summation

**Property 1:**

$$
\sum\limits_{i=1}^n (x_i - \bar{x}) = 0
$$

-   This is because we can expand the left to $\sum x_i - \sum \bar x$.
-   Then, we know $\sum x_i = \sum \bar x$ (by the formula for mean), so $\sum x_i - \sum \bar x = 0$.

**Property 2:**

$$
\sum\limits_{i=1}^n x_i(y_i - \bar{y}) = \sum\limits_{i=1}^n(x_i - \bar{x}) (y_i - \bar{y}) 
$$

-   This is because on the right side can expand to $\sum [x_i(y_i - \bar y) - \bar x (y_i - \bar y)]$.
-   Then, split into $\sum x_i (y_i - \bar y) - \bar x \sum (y_i - \bar y)$.
-   We know that by property 1 (which applies to any variable), $\sum (y_i - \bar y) = 0$. Thus, the right side disappears, and we are left with $\sum x_i (y_i - \bar y)$.

**Property 3:**

$$
\sum\limits_{i=1}^n x_i(x_i - \bar{x}) = \sum\limits_{i=1}^n(x_i - \bar{x})^2
$$

-   Start by expanding right side to $\sum [ x_i ( x_i - \bar x) - \bar x (x_i - \bar x)]$
-   Which splits into $\sum x_i (x_i - \bar x) - \bar x \sum (x_i - \bar x)$
-   By the first property, we know $\sum x_i - \bar x = 0$, so we are only left with $\sum x_i (x_i - \bar x)$
:::

Let us start off with the OLS estimator for simple linear regression, which calculates the $\hat\beta_1$, the relationship between $x$ and $y$ (which we derived previously)

$$
\hat{\beta}_1 = \frac{\sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^n(x_i - \bar{x})^2}
$$

Let us look at the numerator. Let us expand the numerator:

$$
\begin{split}
& \sum\limits_{i=1}^n (x_i - \bar{x})(y_i - \bar{y}) \\
= & \sum\limits_{i=1}^n [(x_i - \bar x)y_i - (x_i - \bar x) \bar y] \\
= & \sum\limits_{i=1}^n(x_i - \bar x)y_i - \sum\limits_{i=1}^n (x_i - \bar x) \bar y \\
= & \sum\limits_{i=1}^n(x_i - \bar x)y_i - \bar y\sum\limits_{i=1}^n (x_i - \bar x) \\
\end{split}
$$

We know that $\sum (x_i - \bar x) = 0$ from the above properties. Thus, we can further simplify to:

$$
\begin{split}
= & \sum\limits_{i=1}^n(x_i - \bar x)y_i - \bar y\sum\limits_{i=1}^n (x_i - \bar x) \\
= & \sum\limits_{i=1}^n(x_i - \bar x)y_i - \bar y(0) \\
= & \sum\limits_{i=1}^n(x_i - \bar x)y_i
\end{split}
$$

Thus, putting the numerator back in, we now we have the equation:

$$
\hat{\beta}_1 = \frac{\sum_{i=1}^n (x_i - \bar{x})y_i}{\sum_{i=1}^n(x_i - \bar{x})^2}
$$

<br />

We know from the Regression Anatomy Theorem, that in multiple linear regression, $\hat\beta_j$ is not the full relationship between $x_j$ and $y$. Instead, it is the relationship of the part of $x_j$ that is uncorrelated with all other explanatory variables, and $y$. So in other words, it is the relationship of $\widetilde{r_{ji}}$ on $y$.

So, since multiple linear regression is the relationship of $\widetilde{r_{ji}}$ on $y$, instead of $x$ on $y$, let us replace the $x$'s in our formula with $\widetilde{r_{ji}}$:

$$
\hat{\beta}_j = \frac{\sum_{i=1}^n (\widetilde{r_{ji}} - E(\widetilde{r_{ji}}))y_i}{\sum_{i=1}^n(\widetilde{r_{ji}} - E(\widetilde{r_{ji}}))^2}
$$

We can actually simplify this more with a property of regression - remember, that the error term of a regression $u$, should be such that $E(u)=0$. We know that $\widetilde{r_{ji}}$ is also the error term of a regression, so, $E(\widetilde{r_{ji}}) = 0$ as well. Plugging that into our equation, we can get the regression anatomy formula for OLS.

$$
\begin{split}
\hat{\beta}_j & = \frac{\sum_{i=1}^n (\widetilde{r_{ji}} - E(\widetilde{r_{ji}}))y_i}{\sum_{i=1}^n(\widetilde{r_{ji}} - E(\widetilde{r_{ji}}))^2} \\
\\
& = \frac{\sum_{i=1}^n (\widetilde{r_{ji}} - 0)y_i}{\sum_{i=1}^n(\widetilde{r_{ji}} - 0)^2} \\
\\
& = \frac{\sum_{i=1}^n \widetilde{r_{ji}} \ y_i}{\sum_{i=1}^n \widetilde {r_{ji}}^2}
\end{split}
$$
:::

<br />

<br />

------------------------------------------------------------------------

# **Unbiasedness of OLS**

### Unbiased Estimators

An estimator is unbiased, if its estimates $\hat\theta_n$ are of the following:

$$
E(\hat\theta_n) = \theta
$$

Or in other words, if we repeatedly sample and use the estimator, on average, the estimates will be equal to the true population value.

We want an estimator that is unbiased. Why?

-   We know that the expectation of a random variable is its "best guess" of its value.
-   We know that estimates $\hat\theta_n$ from an estimator are a random variable called the sampling distribution.
-   Thus, if $E(\hat\theta_n) = \theta$, that means our "best guess" of the estimator value is the true parameter value $\theta$. That means any one estimate $\hat\theta_n$ is on average, correct.

<br />

### Conditions for Simple Linear Regression

The OLS Estimate of $\widehat{\beta_1}$ are unbiased if four conditions are met (gauss-markov conditions):

-   **SLR.1: The regression is linear in parameters.** This means no parameters $\beta_0, \dots , \beta_k$ are multiplied or divided, only added together.
-   **SLR.2: Random Sampling.** All observations are randomly sampled from the same population.
-   **SLR.3: Non-Zero Variance in** $x$. In other words, $S.Var(x) ≠ 0$.
-   **SLR.4: Zero-Conditional Mean**: $E(u|x) = 0$. This implies that the regressor $x$ is not correlated with the error term $u$.

::: {.callout-tip collapse="true" appearance="simple"}
## Proof of Unbiasedness for Simple Linear Regression

We want to show $E(\hat\beta_1) = \beta_1$. Let us start off with the OLS estimator:

$$
\hat{\beta}_1 = \frac{\sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^n(x_i - \bar{x})^2}
$$

::: {.callout-important collapse="true"}
## SLR.3 Variance in $x$

The existence of $\hat{\beta}_1$ is guaranteed by SLR.3 $Var(x) ≠ 0$, since we cannot divide by 0.
:::

::: {.callout-warning collapse="true"}
## Useful Properties of Summation

Before we start, here are a few key properties of summation

**Property 1:**

$$
\sum\limits_{i=1}^n (x_i - \bar{x}) = 0
$$

-   This is because we can expand the left to $\sum x_i - \sum \bar x$.
-   Then, we know $\sum x_i = \sum \bar x$ (by the formula for mean), so $\sum x_i - \sum \bar x = 0$.

**Property 2:**

$$
\sum\limits_{i=1}^n x_i(y_i - \bar{y}) = \sum\limits_{i=1}^n(x_i - \bar{x}) (y_i - \bar{y}) 
$$

-   This is because on the right side can expand to $\sum [x_i(y_i - \bar y) - \bar x (y_i - \bar y)]$.
-   Then, split into $\sum x_i (y_i - \bar y) - \bar x \sum (y_i - \bar y)$.
-   We know that by property 1 (which applies to any variable), $\sum (y_i - \bar y) = 0$. Thus, the right side disappears, and we are left with $\sum x_i (y_i - \bar y)$.

**Property 3:**

$$
\sum\limits_{i=1}^n x_i(x_i - \bar{x}) = \sum\limits_{i=1}^n(x_i - \bar{x})^2
$$

-   Start by expanding right side to $\sum [ x_i ( x_i - \bar x) - \bar x (x_i - \bar x)]$
-   Which splits into $\sum x_i (x_i - \bar x) - \bar x \sum (x_i - \bar x)$
-   By the first property, we know $\sum x_i - \bar x = 0$, so we are only left with $\sum x_i (x_i - \bar x)$
:::

Let us look at the numerator. Let us expand the numerator:

$$
\begin{split}
& \sum\limits_{i=1}^n (x_i - \bar{x})(y_i - \bar{y}) \\
= & \sum\limits_{i=1}^n [(x_i - \bar x)y_i - (x_i - \bar x) \bar y] \\
= & \sum\limits_{i=1}^n(x_i - \bar x)y_i - \sum\limits_{i=1}^n (x_i - \bar x) \bar y \\
= & \sum\limits_{i=1}^n(x_i - \bar x)y_i - \bar y\sum\limits_{i=1}^n (x_i - \bar x) \\
= & \sum\limits_{i=1}^n(x_i - \bar x)y_i - \bar y(0) \\
= & \sum\limits_{i=1}^n(x_i - \bar x)y_i \\
= &\sum\limits_{i=1}^n(x_i - \bar{x})(\beta_0 + \beta_1 x_i + u_i) \\
= & \sum\limits_{i=1}^n(x_i - \bar{x}) \beta_0 + \sum\limits_{i=1}^n(x_i - \bar{x}) \beta_1  x_i + \sum\limits_{i=1}^n (x_i - \bar{x}) u_i \\
= & \beta_0 \sum\limits_{i=1}^n(x_i - \bar{x}) + \beta_1 \sum\limits_{i=1}^n(x_i - \bar{x}) x_i + \sum\limits_{i=1}^n (x_i - \bar{x}) u_i \\
= & 0 + \beta_1 \sum\limits_{i=1}^n(x_i - \bar{x})^2 + \sum\limits_{i=1}^n(x_i - \bar{x})u_i
\end{split}
$$

Now, putting the numerator back into the equation, we simplify:

$$
\begin{split}
\hat{\beta}_1 & = \frac{\beta_1 \sum_{i=1}^n(x_i - \bar{x})^2 + \sum_{i=1}^n(x_i - \bar{x})u_i}{\sum_{i=1}^n(x_i - \bar{x})^2} \\
&  =  \beta_1 + \frac{\sum_{i=1}^n(x_i - \bar{x}) u_i}{\sum_{i=1}^n (x_i - \bar{x})^2} \\
&  = \beta_1 + \sum\limits_{i=1}^n w_i u_i
\end{split}
$$

-   Where $w_i = \frac{x_i - \bar{x}}{\sum (x_i - \bar{x})^2}$, which is a function of random variable $x$. We could also write $w_i$ as $\frac{x_i - \bar{x}}{SST_x}$ (where $SST_x$ is total sum of squares for $x$).

Since $w_i$ is a function of $x$, that means $\hat\beta_1$ is also a function of $x$ (depends on the value of $x$).

<br />

Now we need to find the expectation $E(\hat\beta_1)$. Thus, we have this equation:

$$
\begin{split}
E(\hat\beta_1|x) & = E \left( \beta_1 + \sum\limits_{i=1}^n w_i u_i \bigg| x \right) \\
& = \beta_1 + \sum\limits_{i=1}^nE(w_iu_i|x)
\end{split}
$$

But what does $\sum E(w_iu_i |x)$ equal? This is where our other two Gauss-Markov Conditions come into play.

::: {.callout-important collapse="true"}
## SLR.4 Zero Conditional Mean

The Zero-Conditional Mean assumption says $E(u|x) = 0$.
:::

::: {.callout-important collapse="true"}
## SLR.2 Random Sampling

The error term $u$ in our regression model is some random variable (with its own probability distribution), that can be defined by its expectation $E(u)$.

If we randomly select one observation $i$ from the data, each observation $i$ has an equal chance of being selected. The error term for that observation, $u_i$, should also have the same expectation as the random variable $u$, since each observation $i$ within $u$ has the same chance of being selected.

Thus, random sampling allows us to say $E(u) = E(u_i)$. Random sampling, combined with zero-conditional mean, allows us to say:

$$
E(u|x) = E(u_i | x_i) = E(u_i|x) = 0
$$
:::

This means that:

$$
E(w_i u_i|x) = w_i E(u_i|x) = 0
$$

Now knowing what $E(w_iu_i|x)$ is, let us plug it back into our equation:

$$
\begin{split}
E(\hat\beta_1|x) & = \beta_1 + \sum\limits_{i=1}^nE(w_iu_i|x) \\
& = \beta_1 + \sum\limits_{i=1}^n0 \\
& = \beta_1
\end{split}
$$

::: {.callout-warning collapse="true" appearance="simple"}
## Law of Iterated Expectations

Law of iterated expectations states:

$E(x) = E[E(x|y)]$
:::

We have solved for $E(\hat\beta_1 |x)$, and not $E (\hat\beta_1)$. We use the Law of Iterated Expectation to conclude:

$$
\begin{split}
E(\hat\beta_1) & = E[E(\hat\beta_1|x)] \\
& = E(\beta_1) \\
& = \beta_1
\end{split}
$$

Thus, $E(\hat\beta_1) = \beta_1$, proving the unbiasedness of OLS under the 4 conditions.
:::

<br />

### Conditions for Multiple Linear Regression

The OLS Estimate of $\widehat{\beta_j}$ are unbiased if four conditions are met (gauss-markov conditions):

-   **MLR.1: The regression is linear in parameters.** This means no parameters $\beta_0, \dots , \beta_k$ are multiplied or divided, only added together.
-   **MLR.2: Random Sampling.** All observations are randomly sampled from the same population.
-   **MLR.3: No Perfect Multicollinearity.** In other words, $\sum \widetilde{r_{ji}}^2 ≠ 0$ - there is no perfect correlation between any regressor and any function of other regressors.
-   **SLR.4: Zero-Conditional Mean**: $E(u|x_1, \dots, x_k) = 0, \ \forall \ x1, \dots , x_k$. This implies that no regressor, and no function of any combination of regressors, are correlated with the error term.

::: {.callout-tip collapse="true" appearance="simple"}
## Proof of Unbiasedness for Multiple Linear Regression

For simplicity, let us focus on $\hat\beta_1$. However, this can be generalised to any $\hat\beta_2, \dots, \hat\beta_k$. Recall the regression anatomy solution of OLS for $\hat\beta_1$:

$$
\hat\beta_1 = \frac{\sum_{i=1}^n \widetilde{r_{1i}} \ y_i}{\sum_{i=1}^n \widetilde{r_{1i}}^2}
$$

-   Where $\widetilde{r_{1i}}$ is the part of $x_1$ uncorrelated with $x_2, \dots, x_k$.

::: {.callout-tip collapse="true"}
## MLR.3 No Perfect Multicollinearity

The existence of $\hat\beta_1$ is guaranteed by MLR.3 $\sum\widetilde{r_{1i}}^2 ≠ 0$, since we cannot divide by 0.
:::

Now, let us plug in $y_i = \beta_0 + \beta_1 x_{1i} + \dots + \beta_k x_{ki} + u_i$ into our regression anatomy formula:

$$
\begin{split}
\hat\beta_1 & = \frac{\sum_{i=1}^n \widetilde{r_{1i}} \ y_i}{\sum_{i=1}^n \widetilde{r_{1i}}^2} \\
& = \frac{\sum_{i=1}^n \widetilde{r_{1i}} \ (\beta_0 + \beta_1 x_{1i} + \dots + \beta_k x_{ki} + u_i)}{\sum_{i=1}^n \widetilde{r_{1i}}^2} \\
\end{split}
$$

::: {.callout-warning collapse="true"}
## Useful Properties of Summations

Before we start, here are a few key properties of summation

**Property 1:**

$$
\sum\limits_{i=1}^n \widetilde{r_{1i}} = 0
$$

-   This is because $\widetilde{r_{1i}}$ is a residual term of a OLS regression of outcome $x_1$ and explanatory variables $x_2, \dots, x_k$, and we know OLS residuals sum to 0.

**Property 2:**

$$
\sum\limits_{i=1}^n \widetilde{r_{1i}} x_{ji} = 0, \text{ given } j=2, \dots, k 
$$

-   Because for OLS, $\sum x_i \hat u_i = 0$, and we know $\widetilde{r_{1i}}$ is the residual $\hat u_i$ in a regression with explanatory variables $x_2, \dots, x_k$ and outcome variable $x_1$.

**Property 3:**

$$
\sum\limits_{i=1}^n \widetilde{r_{1i}} x_{1i} = \sum\limits_{i=1}^n \widetilde{r_{1i}}(\hat x_{1i} + \sum\limits_{i=1}^n \widetilde{r_{1i}}) = \sum\limits_{i=1}^n \widetilde{r_{1i}}^2
$$

-   Because we have the regression fitted values $\hat x_{1i} = \hat \gamma_0 + \hat\gamma_1 x_2 + \dots + \hat\gamma_{k-1} x_{k} + \widetilde{r_{1i}}$ from regression anatomy.
-   And we know with regression, actual values are the predicted plus residual: $y_i = \hat y_i + \hat u_i$. Thus, $x_i = \hat x_i + \widetilde{r_{1i}}$.
:::

Now, focusing on the numerator, and using the summation properties above, let us simplify:

$$
\begin{split}
& \sum\limits_{i=1}^n \widetilde{r_{1i}} \ (\beta_0 + \beta_1 x_{1i} + \dots + \beta_k x_{ki} + u_i) \\
& = \sum\limits_{i=1}^n (\widetilde{r_{1i}}\beta_0 + \widetilde{r_{1i}}\beta_1x_{1i} + \dots + \widetilde{r_{1i}}\beta_k x_{ki} + \widetilde{r_{1i}} u_i) \\
& = \sum\limits_{i=1}^n \widetilde{r_{1i}}\beta_0 + \sum\limits_{i=1}^n \widetilde{r_{1i}}\beta_1 x_{1i} + \dots + \sum\limits_{i=1}^n \widetilde{r_{1i}}\beta_k x_{ki} + \sum\limits_{i=1}^n \widetilde{r_{1i}} u_i \\
& = \beta_0 \sum\limits_{i=1}^n \widetilde{r_{1i}} + \beta_1 \sum\limits_{i=1}^n \widetilde{r_{1i}} x_{1i} + \dots + \beta_k \sum\limits_{i=1}^n \widetilde{r_{1i}} x_{ki} + \sum\limits_{i=1}^n \widetilde{r_{1i}} u_i \\
& = \beta_0 (0) + \beta_1 \sum\limits_{i=1}^n \widetilde{r_{1i}}^2 + \beta_2 (0) + \dots + \beta_k (0) + \sum\limits_{i=1}^n \widetilde{r_{1i}} u_i \\
& = \beta_1 \sum\limits_{i=1}^n \widetilde{r_{1i}}^2 + \sum\limits_{i=1}^n \widetilde{r_{1i}} u_i
\end{split}
$$

Now, putting the numerator back in, we can simplify:

$$
\begin{split}
\hat\beta_1 & = \frac{\beta_1 \sum_{i=1}^n \widetilde{r_{1i}}^2 + \sum_{i=1}^n \widetilde{r_{1i}} u_i}{\sum_{i=1}^n \widetilde{r_{1i}}^2} \\
& \\
& = \frac{\beta_1 \sum_{i=1}^n \widetilde{r_{1i}}^2}{\sum_{i=1}^n \widetilde{r_{1i}}^2} + \frac{\sum_{i=1}^n \widetilde{r_{1i}} u_i}{\sum_{i=1}^n \widetilde{r_{1i}}^2} \\
& \\
& = \beta_1 + \frac{\sum_{i=1}^n \widetilde{r_{1i}} u_i}{\sum_{i=1}^n \widetilde{r_{1i}}^2}
\end{split}
$$

<br />

Now, we want to find $E(\hat\beta_1)$. Note that the second part of the equation is a function of $u_i$, of which itself is a function of all explanatory variables $x_{1i}, \dots, x_{ki}$. Thus, we know:

$$
\begin{split}
E(\hat\beta_1 | x_{1i}, \dots x_{ki}) & = \beta_1 + E\left( \frac{\sum_{i=1}^n \widetilde{r_{1i}} u_i}{\sum_{i=1}^n \widetilde{r_{1i}}^2} \biggr|x_{1i},\dots, x_{ki} \right) \\
& = \beta_1 + \frac{\sum_{i=1}^n \widetilde{r_{1i}} \ E(u_i|x_{1i}, \dots,x_{ki})}{\sum_{i=1}^n \widetilde{r_{1i}}^2}
\end{split}
$$

But what is $E(u_i|x_{1i}, \dots , x_{ki})$? We can use two Gauss-Markov conditions to evaluate this.

::: {.callout-tip collapse="true"}
## MLR.4 Zero Conditional Mean

The Zero-Conditional Mean assumption says $E(u|x_1, \dots, x_k) = 0$.
:::

::: {.callout-tip collapse="true"}
## MLR.2 Random Sampling

The error term $u$ in our regression model is some random variable (with its own probability distribution), that can be defined by its expectation $E(u)$.

If we randomly select one observation $i$ from the data, each observation $i$ has an equal chance of being selected. The error term for that observation, $u_i$, should also have the same expectation as the random variable $u$, since each observation $i$ within $u$ has the same chance of being selected.

Thus, random sampling allows us to say $E(u) = E(u_i)$. Random sampling, combined with zero-conditional mean, allows us to say:

Random sampling, combined with Zero-Conditional Mean, allows us to say:

$$
E(u|x_1, \dots, x_k)=E(u_1|x_{1i}, \dots, x_{ki}) = 0
$$
:::

Thus, plugging that in to our formula, we get:

$$
\begin{split}
E(\hat\beta_1 | x_{1i}, \dots x_{ki}) 
& = \beta_1 + \frac{\sum_{i=1}^n \widetilde{r_{1i}} \ E(u_i|x_{1i}, \dots,x_{ki})}{\sum_{i=1}^n \widetilde{r_{1i}}^2} \\
& = \beta_1 + \frac{\sum_{i=1}^n \widetilde{r_{1i}} \ (0)}{\sum_{i=1}^n \widetilde{r_{1i}}^2} \\
& = \beta_1 + 0 \\
& = \beta_1
\end{split}
$$

::: {.callout-warning collapse="true" appearance="simple"}
## Law of Iterated Expectations

Law of iterated expectations states:

$E(x) = E[E(x|y)]$
:::

Now, just like in simple linear regression, we use the **law of iterated expectations** to conclude this proof:

$$
\begin{split}
E(\hat\beta_1) & = E[E(\hat\beta_1|x_{1i}, \dots, x_{ki})] \\
& = E(\beta_1) \\
& = \beta_1
\end{split}
$$

Thus, OLS is unbiased under these 4 conditions.
:::

<br />

<br />

------------------------------------------------------------------------

# **Consistency of OLS**

### Asymptotically Consistent Estimators

Asymptotic properties are properties of estimators as the sample size becomes larger and larger. Or more mathematically, as the sample size $n$ approaches infinity.

An estimator is consistent, if as we increase sample size towards infinity, the estimate will become more and more concentrated around the true population value $\theta$.

Or in other words, as sample size increases indefinitely, we will get closer and closer to the true population value $\theta$, until at infinite sample size, all our estimates will be exactly $\theta$. Mathematically:

$$
Pr(|\hat\theta_n - \theta|> \epsilon) \rightarrow 0, \text { as } n \rightarrow ∞
$$

-   Or in other words, the proabability that the distance between an estimate $\hat\theta_n$ and the true population value $\theta$ will be higher than a small close-to-zero value $\epsilon$ will be 0, since our estimates $\hat\theta_n$ will converge at the $\theta$.

::: {.callout-tip collapse="true" appearance="simple"}
## Biased but Consistent

An estimator can be both biased, but consistent.

-   i.e. in smaller sample sizes, the estimator might not be on average correct, but over a large enough sample size, it will become "unbiased".

For example, in the figure below, we can see that this estimator is biased at small values of $n$, but as $n$ increases, it becomes more consistent, collapsing its distribution around the true $\theta$.

![](images/clipboard-1215503621.png){fig-align="center" width="70%"}
:::

::: {.callout-tip collapse="true" appearance="simple"}
## Law of Large Numbers and Consistency

The law of large numbers states that the sample average of a random sample, is a consistent estimator of the population mean.

For example, let us say we have a random variable $x$. We take a random sample of $n$ units, so our sample is $(x_1, \dots, x_n)$.

-   Let us define $\bar x_n$ as our sample average.
-   Let us define $\mu$ as the true population mean of variable $x$.

The law of large numbers states that:

$$
\text{plim}( \bar x_n) = \mu
$$

-   Where $\text{plim}$ states that as $n$ approaches infinity, the probability distribution of $\bar x_n$ collapses around $\mu$.

<br />

Why is this the case? This sample mean estimator is calculated simply through the formula for mean:

$$
\bar x_n = \frac{1}{n}\sum\limits_{i=1}^n x_i
$$

Let us define the variance of our sample of $x_1, \dots, x_n$ as $Var(x_i) = \sigma^2$. We can now find the variance of our sampling distribution of estimator $\bar x_n$:

$$
\begin{split}
Var(\bar x_n) & = Var\left( \frac{1}{n}\sum\limits_{i=1}^n x_i \right) \\
& = \frac{1}{n^2} Var \left(\sum\limits_{i=1}^n x_i\right) \\
& = \frac{1}{n^2} \sum\limits_{i=1}^n Var(x_i) \\
& = \frac{1}{n^2} \sigma^2 \\
& = \frac{\sigma^2}{n}
\end{split}
$$

And as sample size $n$ increases to infinity, we get:

$$
\lim\limits_{n \rightarrow ∞} Var(\bar x_n) = \lim\limits_{n \rightarrow ∞} \frac{\sigma^2}{n} = 0
$$

Thus, the variance of our estimator $\bar x_n$ shrinks to zero, so as sample size increases to infinity $n$, the sampling distribution of estimator $\bar x_n$ collapses around the true population mean.
:::

<br />

### Conditions for OLS Consistency

OLS is consistent (but possibly biased) under a set of 4 conditions. The first 3 conditions are the same as the unbiasedness conditions, but the 4th condition is a weakened version of SLR.4/MLR.4, that we denote as SLR.4'/MLR.4':

-   **SLR.1/MLR.1: The regression is linear in parameters.** This means no parameters $\beta_0, \dots , \beta_k$ are multiplied or divided, only added together.
-   **SLR.2/MLR.2: Random Sampling.** All observations are randomly sampled from the same population.
-   **SLR.3/MLR.3: Variance in** $x$ / **No Perfect Multicollinearity.**
-   **SLR.4'/MLR'4: Zero-Mean and Exogeneity.** This means $E(u) = 0$ and $Cov(x_j, u) = 0$ for each $x_j$ (also often written as $E(x_j u)=0$).

The SLR.4'/MLR.4' is weaker than the unbiased condition, because it no longer requires any function of explanatory variables to be uncorrelated with the error. It now only requires each individual explanatory variable to be uncorrelated with the error term.

::: {.callout-tip collapse="true" appearance="simple"}
## Proof of OLS Asymptotic Consistency

::: {.callout-warning collapse="true"}
## Useful Properties of Summation

Before we start, here are a few key properties of summation

**Property 1:**

$$
\sum\limits_{i=1}^n (x_i - \bar{x}) = 0
$$

-   This is because we can expand the left to $\sum x_i - \sum \bar x$.
-   Then, we know $\sum x_i = \sum \bar x$ (by the formula for mean), so $\sum x_i - \sum \bar x = 0$.

**Property 2:**

$$
\sum\limits_{i=1}^n x_i(y_i - \bar{y}) = \sum\limits_{i=1}^n(x_i - \bar{x}) (y_i - \bar{y}) 
$$

-   This is because on the right side can expand to $\sum [x_i(y_i - \bar y) - \bar x (y_i - \bar y)]$.
-   Then, split into $\sum x_i (y_i - \bar y) - \bar x \sum (y_i - \bar y)$.
-   We know that by property 1 (which applies to any variable), $\sum (y_i - \bar y) = 0$. Thus, the right side disappears, and we are left with $\sum x_i (y_i - \bar y)$.

**Property 3:**

$$
\sum\limits_{i=1}^n x_i(x_i - \bar{x}) = \sum\limits_{i=1}^n(x_i - \bar{x})^2
$$

-   Start by expanding right side to $\sum [ x_i ( x_i - \bar x) - \bar x (x_i - \bar x)]$
-   Which splits into $\sum x_i (x_i - \bar x) - \bar x \sum (x_i - \bar x)$
-   By the first property, we know $\sum x_i - \bar x = 0$, so we are only left with $\sum x_i (x_i - \bar x)$
:::

Let us look at the numerator. Let us expand the numerator:

$$
\begin{split}
& \sum\limits_{i=1}^n (x_i - \bar{x})(y_i - \bar{y}) \\
= & \sum\limits_{i=1}^n [(x_i - \bar x)y_i - (x_i - \bar x) \bar y] \\
= & \sum\limits_{i=1}^n(x_i - \bar x)y_i - \sum\limits_{i=1}^n (x_i - \bar x) \bar y \\
= & \sum\limits_{i=1}^n(x_i - \bar x)y_i - \bar y\sum\limits_{i=1}^n (x_i - \bar x) \\
= & \sum\limits_{i=1}^n(x_i - \bar x)y_i - \bar y(0) \\
= & \sum\limits_{i=1}^n(x_i - \bar x)y_i \\
= &\sum\limits_{i=1}^n(x_i - \bar{x})(\beta_0 + \beta_1 x_i + u_i) \\
= & \sum\limits_{i=1}^n(x_i - \bar{x}) \beta_0 + \sum\limits_{i=1}^n(x_i - \bar{x}) \beta_1  x_i + \sum\limits_{i=1}^n (x_i - \bar{x}) u_i \\
= & \beta_0 \sum\limits_{i=1}^n(x_i - \bar{x}) + \beta_1 \sum\limits_{i=1}^n(x_i - \bar{x}) x_i + \sum\limits_{i=1}^n (x_i - \bar{x}) u_i \\
= & 0 + \beta_1 \sum\limits_{i=1}^n(x_i - \bar{x})^2 + \sum\limits_{i=1}^n(x_i - \bar{x})u_i
\end{split}
$$

Now, putting the numerator back into the equation, we simplify:

$$
\begin{split}
\hat{\beta}_1 & = \frac{\beta_1 \sum_{i=1}^n(x_i - \bar{x})^2 + \sum_{i=1}^n(x_i - \bar{x})u_i}{\sum_{i=1}^n(x_i - \bar{x})^2} \\
&  =  \beta_1 + \frac{\sum_{i=1}^n(x_i - \bar{x}) u_i}{\sum_{i=1}^n (x_i - \bar{x})^2}
\end{split}
$$

We can add $\frac{1}{n}$ (or $n^{-1}$) to the top and bottom of the fraction (which cancel each other out, keeping the equation equivalent):

$$
\hat\beta_1=  \beta_1 + \frac{n^{-1} \sum_{i=1}^n(x_i - \bar{x}) u_i}{n^{-1} \sum_{i=1}^n (x_i - \bar{x})^2}
$$

Now, let us expand the numerator, and simplify, and we get:

$$
\begin{split}
\hat\beta_1 & =  \beta_1 + \frac{n^{-1} \sum_{i=1}^n x_i u_i - n^{-1} \sum_{i=1}^n \bar x u_i}{n^{-1} \sum_{i=1}^n (x_i - \bar{x})^2} \\
& =  \beta_1 + \frac{n^{-1} \sum_{i=1}^n x_i u_i - \bar x \ n^{-1} \sum_{i=1}^n u_i}{n^{-1} \sum_{i=1}^n (x_i - \bar{x})^2} \\
& =  \beta_1 + \frac{ \overline{xu} - \bar x \bar u}{S.Var(x)}
\end{split}
$$

We want to find $\text{plim}(\hat\beta_1)$. We will need a few properties for this:

::: {.callout-warning collapse="true"}
## Properties of Probability Limits

We know these general rules about probability limits (derived from the law of large numbers):

$$
\begin{split}
& \text{plim}(\bar x_n) = \mu _x \\
& \text{plim}(S.Var(x_i)) = Var(x_i) \\
& \text{plim}(S.Cov(x_i, y_i)) = Cov (x_i, y_i)
\end{split}
$$

The other properties are about algebra with probability limits. Assume $\text{plim} (u_n) = a$, and $\text{plim}(v_n) = b$. Then, the following are true:

$$
\begin{split}
& \text{plim} (u_n + v_n) = a + b \\
& \text{plim} (u_n v_n) = ab \\
& \text{plim} (u_n v_n) = a/b
\end{split}
$$
:::

Knowing this, we then know that:

$$
\begin{split}
\text{plim}(\hat\beta_1) & = \beta_1 + \frac{ \text{plim}(\overline{xu}) - \text{plim}( \bar x \bar u) }{ \text{plim}(S.Var(x))} \\
& = \beta_1 + \frac{ \text{plim}(\overline{xu}) - \text{plim}(\bar x) \text{plim}(\bar u) }{ \text{plim}(S.Var(x))} \\
&  = \beta_1 + \frac{ E(xu) - E(x) E(u) }{Var(x)} \\
\end{split}
$$

::: callout-important
## MLR.4' Zero-Mean and Exogeneity Assumption

The weakened Zero-Mean and Exogeneity assumption states:

$$
\begin{split}
& E(u) = 0 \\
& E(xu) = 0
\end{split}
$$
:::

Using this assumption, we can conclude the proof:

$$
\begin{split}
\text{plim}(\hat\beta_1) & = \beta_1 + \frac{ E(xu) - E(x) E(u) }{Var(x)} \\
& = \beta_1 + \frac{ 0 - E(x)0}{Var(x)} \\
& = \beta_1 + \frac{ 0 }{Var(x)} \\
& = \beta_1+0 \\
\text{plim}(\hat\beta_1) & = \beta_1
\end{split}
$$

Thus, OLS is asymptotically consistent under a weakened version of MLR.4 Zero-Conditional Mean - called the Zero-Mean and Exogeneity Assumption.

-   Under this weakened assumption (without meeting the full MLR.4 assumption), OLS is [biased but consistent]{.underline}.
:::

<br />

### Exogeneity

Exogeneity is defined as no covariance between any regressor and the error term:

$$
Cov(x_j, u) = 0 \quad \text{or} \quad E(x_j u) = 0
$$

When exogeneity is not met, we have **endogeneity**. Any regressor that is correlated with the error term is considered an **endogenous regressor**.

Exogeneity is required to be met for both MLR.4 and MLR.4', so if we have endogeneity, that means OLS is neither unbiased or consistent.

-   Because of this, exogeneity is the "critical assumption" of using OLS for causal estimation.

Exogeneity is typically caused by either omitted confounders from the regression, reverse causality, or measurement error.

-   To address these issues, we will need strong causal identification designs (such as IV, regression discontinuity, diff-in-diff, or randomisation).

<br />

<br />

------------------------------------------------------------------------

# **Variance of OLS**

### Homoscedastic Standard Errors

Homoscedasticity says that The error term has the same variance given any value of $x$. Mathematically:

$$
Var(u|x_1, \dots, x_k) = \sigma^2 \text{ for all } x
$$

When homoscedasticity (SLR.5/MLR.5) is true, and the 4 conditions of OLS unbiasedness are met, OLS is considered to be the **best linear unbiased estimator**. This means OLS is the linear unbiased estimator with the lowest variance.

The OLS standard errors for simple linear regression are:

$$
\widehat{se}(\hat\beta_1) = \frac{\hat\sigma}{\sqrt{SST_x}} \text{ where } \hat\sigma^2 = \frac{SSR}{n-2} = \frac{\sum_{i=1}^n \hat u_i^2}{n-2}
$$

::: {.callout-tip collapse="true" appearance="simple"}
## Deriving Standard Errors for Simple Linear Regression

Remember when we were proving unbiasedness of OLS, we got to this stage:

$$
\hat\beta_1 = \beta_1 + \sum\limits_{i=1}^n w_i u_i
$$

-   Where $w_i = \frac{x_i - \bar{x}}{\sum (x_i - \bar{x})^2}$, which is a function of random variable $x$.
-   We could also write $w_i$ as $\frac{x_i - \bar{x}}{SST_x}$ (where $SST_x$ is total sum of squares for $x$).

We know that $\beta_1$ is a constant (the true value in the population), so that never changes. Thus, it cannot be the variance in $\hat\beta_1$. Thus, $\sum w_i u_i$ is the variance in $\hat\beta_1$.

$$
\begin{split}
Var(\hat\beta_1|x) & = Var\left( \sum\limits_{i=1}^n w_i u_i \bigg| x\right) \\
& = \sum\limits_{i=1}^n Var(w_i u_i | x) \\
& = \sum\limits_{i=1}^n w_i^2 Var(u_i | x)
\end{split}
$$

And given SLR.2 Random Sampling, we know $Var(u_i | x)$ is also equal to $Var(u_i|x_i)$. Thus:

$$
Var(\hat\beta_1|x) = \sum\limits_{i=1}^n w_i^2 Var(u_i | x_i)
$$

And using SLR.5 homoscedasticity, we know $Var(u|x) = \sigma^2$ and is constant, thus:

$$
\begin{split}
Var(\hat\beta_1 | x) & = \sum\limits_{i=1}^n w_i^2 Var(u_i | x_i) \\
& = \sum\limits_{i=1}^n w_i^2 \sigma^2 \\
& = \sigma^2 \sum\limits_{i=1}^n w_i^2
\end{split}
$$

Remember, $w_i$ is its own function of $x$, where $w_i = \frac{x_i - \bar{x}}{\sum (x_i - \bar{x})^2}$, or can be written as $\frac{x_i - \bar{x}}{SST_x}$.

We have $\sum w_i^2$ in our final equation, and we can do some quick algebra to rearrange it.

$$
\begin{split}
\sum\limits_{i=1}^n w_i^2 & = \sum\limits_{i=1}^n \frac{(x_i - \bar{x})^2}{(SST_x)^2} \\
& = \frac{\sum_{i=1}^n(x_i - \bar x)^2}{(SST_x)^2} \\
& = \frac{SST_x}{(SST_x)^2} \\
& = \frac{1}{SST_x}
\end{split}
$$Thus, we can plug that in to get our final variance of $\hat\beta_1$ formula:

$$
\begin{split}
Var(\hat\beta|x) & = \sigma^2 \sum\limits_{i=1}^n w_i^2 \\
& = \sigma^2 \frac{1}{SST_x} \\
& = \frac{\sigma^2}{SST_x}
\end{split}
$$

Thus, that is the variance of our OLS estimator $\hat\beta_1$, and also the variance of the sampling distribution of $\hat\beta_1$.

::: {.callout-warning collapse="true" appearance="simple"}
## Estimating $\sigma^2$

We have $\sigma^2$ in our formula. Assuming Homoscedasticity is met, we know $Var(u|x) = \sigma^2$.

What is variance? We know that the formula for variance is:

$$
E(x - \mu)^2
$$

We know from SLR.4 Zero-Conditional Mean assumption that $E(u|x) = 0$. Thus, we can use that to calculate the variance $Var(u|x)$ using the variance formula:

$$
\begin{split}
Var(u|x) = \sigma^2 & = E[ \ ((u|x) - E(u|x))^2 \ ] \\
& = E((u|x) - 0)^2 \\
& = E[(u|x)^2] \\
& = E(u^2 |x)
\end{split}
$$

And since by homoscedasticity, we know variance does not depend on $x$, so $Var(u|x) = Var(u)$. Thus, we also know that:

$$
E(u^2|x) = E(u^2) = \sigma^2
$$

However, we do not actually know the value of $E(u^2)$! Remember, that is the error term $u$ - while we only know the residual term $\hat u$.

So, what we can do is well, simply replace $u$ with its estimate, $\hat u$.

-   Recall that $u_i = y_i - \beta_0 - \beta_1 x_i$
-   And $\hat u_i = y_i - \hat\beta_0 - \hat\beta_1 x_i$.

So naturally, instead of $\sigma^2 = E(u^2)$, we could estimate it with $E(\hat u^2)$. Mathematically:

$$
\begin{split}
\hat\sigma^2 = E(\hat u^2) & = \frac{1}{n} \sum\limits_{i=1}^n \hat u_i^2 \\
& = SSR/n
\end{split}
$$

-   Where $SSR$ is the square sum of residuals

However, there is an issue with this estimate of $\sigma^2$. It is biased - the expected value is actually slightly less than $\sigma^2$.

We will not prove this mathematically, but this bias is because OLS imposes two conditions on its estimation process (that were discussed in the OLS algebraic properties):

$$
\begin{split}
& \sum\limits_{i=1}^n \hat u_i = 0 \\
& \sum\limits_{i=1}^n x_i \hat u_i = 0
\end{split}
$$

-   The actual error term $u_i$ (not the OLS residuals $\hat u_i$) do not have these restrictions.

We can adjust the estimator to be more accurate by including a degrees of freedom adjustment. So, instead of $SSR/n$, we can do $SSR/(n-2)$. Thus, our estimator for $\sigma^2$ is:

$$
\hat\sigma^2 = \frac{SSR}{n-2} = \frac{\sum_{i=1}^n \hat u_i^2}{n-2} 
$$
:::

With that estimate of $\sigma^2$, we can plug it back into our formula for the standard deviation of $\hat\beta_1$ (standard deviation is the square root of variance). This is our standard error:

$$
\widehat{se}(\hat\beta_1) = \frac{\hat\sigma}{\sqrt{SST_x}} \text{ where } \hat\sigma^2 = \frac{SSR}{n-2} = \frac{\sum_{i=1}^n \hat u_i^2}{n-2}
$$
:::

The OLS standard errors for multiple linear regressions are:

$$
\widehat{se}(\hat\beta_j) = \frac{\hat\sigma}{\sqrt{\sum_{i=1}^n \widetilde{r_{1i}}^2}} \text{ where } \hat\sigma = \sqrt{\frac{SSR}{n-k-1}} = \sqrt{\frac{\sum_{i=1}^n \hat u_i^2}{n-k-1}}
$$

::: {.callout-tip collapse="true" appearance="simple"}
## Deriving Standard Errors for Multiple Linear Regression

Let us find the variance of OLS estimates (we will use $\hat\beta_1$ for simplicity, but this applies to any other coefficient $\hat\beta_1 , \dots, \hat\beta_k$. In proving unbiasedness of OLS, we got to this stage:

$$
\hat\beta_1 = \beta_1 + \frac{\sum_{i=1}^n \widetilde{r_{1i}} u_i}{\sum_{i=1}^n \widetilde{r_{1i}}^2}
$$

We know that $\beta_1$ is a constant (the true value in the population), so that never changes. Thus, it cannot be the variance in $\hat\beta_1$. Thus, the second part is the variance in $\hat\beta_1$.

Let us define $w_i$ as following, as a function of $x_1, \dots, x_k$:

$$
w_i = \frac{\widetilde{r_{1i}}}{\sum_{i=1}^n\widetilde{r_{1i}}^2}
$$

This allows us to write $\hat\beta_1$ as:

$$
\hat\beta_1 = \beta_1 + \sum\limits_{i=1}^nw_i u_i
$$

<br />

Thus, we can proceed in the same way as the simple linear regression case:

$$
\begin{split}
Var(\hat\beta_1|x_1, \dots, x_k) & = Var\left( \sum\limits_{i=1}^nw_i u_i \biggr|x_1, \dots ,x_k \right) \\ & = \sum\limits_{i=1}^n Var(w_i u_i | x_1, \dots, x_k) \\
& = \sum\limits_{i=1}^n w_i^2 \ Var(u_i | x_1, \dots, x_k) \\
& = \sum\limits_{i=1}^n w_i^2 \sigma^2 \\
& = \sigma^2 \sum\limits_{i=1}^n w_i^2
\end{split}
$$

Now, plugging back in $w_i$, we get:

$$
\begin{split}
Var(\hat\beta_1|x_1, \dots, x_k) & = \sigma^2 \sum\limits_{i=1}^n w_i^2 \\
& = \sigma^2 \sum_{i=1}^n \left( \frac{\widetilde{r_{1i}}}{\sum_{i=1}^n\widetilde{r_{1i}}^2} \right) \\
& = \sigma^2 \frac{\sum_{i=1}^n \widetilde{r_{1i}}}{\sum_{i=1}^n\widetilde{r_{1i}}^2} \\
& = \frac{\sigma^2}{\sum_{i=1}^n \widetilde{r_{1i}}^2}
\end{split}
$$

And assuming homoscedasticity (where $Var(\hat\beta_1)$ does not depend on $x_1, \dots, x_k$, we thus know:

$$
Var(\hat\beta_1) = \frac{\sigma^2}{\sum_{i=1}^n \widetilde{r_{1i}}^2}
$$

::: {.callout-warning collapse="true" appearance="simple"}
## Estimating $\sigma^2$

We have $\sigma^2$ in our formula. Assuming Homoscedasticity is met, we know $Var(u|x) = \sigma^2$.

What is variance? We know that the formula for variance is:

$$
E(x - \mu)^2
$$

We know from SLR.4 Zero-Conditional Mean assumption that $E(u|x) = 0$. Thus, we can use that to calculate the variance $Var(u|x)$ using the variance formula:

$$
\begin{split}
Var(u|x) = \sigma^2 & = E[ \ ((u|x) - E(u|x))^2 \ ] \\
& = E((u|x) - 0)^2 \\
& = E[(u|x)^2] \\
& = E(u^2 |x)
\end{split}
$$

And since by homoscedasticity, we know variance does not depend on $x$, so $Var(u|x) = Var(u)$. Thus, we also know that:

$$
E(u^2|x) = E(u^2) = \sigma^2
$$

However, we do not actually know the value of $E(u^2)$! Remember, that is the error term $u$ - while we only know the residual term $\hat u$.

So, what we can do is well, simply replace $u$ with its estimate, $\hat u$.

-   Recall that $u_i = y_i - \beta_0 - \beta_1 x_i$
-   And $\hat u_i = y_i - \hat\beta_0 - \hat\beta_1 x_i$.

So naturally, instead of $\sigma^2 = E(u^2)$, we could estimate it with $E(\hat u^2)$. Mathematically:

$$
\begin{split}
\hat\sigma^2 = E(\hat u^2) & = \frac{1}{n} \sum\limits_{i=1}^n \hat u_i^2 \\
& = SSR/n
\end{split}
$$

-   Where $SSR$ is the square sum of residuals

However, there is an issue with this estimate of $\sigma^2$. It is biased - the expected value is actually slightly less than $\sigma^2$.

We will not prove this mathematically, but this bias is because OLS imposes two conditions on its estimation process (that were discussed in the OLS algebraic properties):

$$
\begin{split}
& \sum\limits_{i=1}^n \hat u_i = 0 \\
& \sum\limits_{i=1}^n x_i \hat u_i = 0
\end{split}
$$

-   The actual error term $u_i$ (not the OLS residuals $\hat u_i$) do not have these restrictions.

We can adjust the estimator to be more accurate by including a degrees of freedom adjustment. So, instead of $SSR/n$, we can do $SSR/(n-k-1)$. Thus, our estimator for $\sigma^2$ is:

$$
\hat\sigma^2 = \frac{SSR}{n-k-1} = \frac{\sum_{i=1}^n \hat u_i^2}{n-k-1}
$$
:::

With that estimate of $\sigma^2$, we can plug it back into our formula for the standard deviation of $\hat\beta_1$ (standard deviation is the square root of variance). This is our standard error:

$$
\widehat{se}(\hat\beta_j) = \frac{\hat\sigma}{\sqrt{\sum_{i=1}^n \widetilde{r_{1i}}^2}} \text{ where } \hat\sigma = \sqrt{\frac{SSR}{n-k-1}} = \sqrt{\frac{\sum_{i=1}^n \hat u_i^2}{n-k-1}}
$$
:::

<br />

### Robust Standard Errors

When heteroscedasticity is present, OLS is no longer the linear unbiased estimator with the lowest variance.

This isn't that important. However, what is important is that we have to use a different standard error formula when heteroscedasticity is present.

The robust standard errors for simple linear regression are:

$$
\widehat{se}(\hat\beta_1|x) = \sqrt{\frac{\sum_{i=1}^n (x_i - \bar x)^2 \hat u^2_i}{SST_x^2}}
$$

::: {.callout-tip collapse="true" appearance="simple"}
## Deriving Robust Standard Errors for Simple Linear Regression

Without MLR.5 Homoscedasticity, variance $Var(u|x)$ is no longer constant at $\sigma^2$. Instead, $Var(u|x)$ varies depending on the value of $x$. Let us define the variance as:

$$
Var(u_i|x_i) = \sigma_i^2
$$

-   Where $\sigma^2_1$ is the error term variance for $x_1$, and $\sigma_2^2$ is the error term variance for $x_2$, and so on $\sigma_3^2, \dots, \sigma_n^2$.

Then, recalling from the the OLS unbiased proof, we have the following formula for the simple linear regression $\hat\beta_1$ estimate:

$$
\hat\beta_1 = \beta_1 + \sum_{i=1}^nw_i u_i
$$

-   Where $w_i = \frac{x_i - \bar x}{\sum(x_i - \bar x)^2}$, which is a function of random variable $x$.
-   We could also write $w_i$ as $\frac{x_i - \bar x}{SST_x}$.

We know that $\beta_1$ is a constant (the true value in the population), so that never changes. Thus, it cannot be the variance in $\hat\beta_1$. Thus, the second part is the variance in $\hat\beta_1$. Thus:

$$
\begin{split}
Var(\hat\beta_1|x) & = Var \left( \sum\limits_{i=1}^n w_i u_i \biggr| x \right) \\
& = \sum\limits_{i=1}^n Var(w_i u_i |x) \\
& = \sum\limits_{i=1}^n w_i^2 \ Var (u_i | x) \\
& = \sum\limits_{i=1}^nw_i^2 \ Var(u_i|x_i) \\
& = \sum\limits_{i=1}^n w_i^2 \sigma_i^2
\end{split}
$$

Now, plugging back $w_i$ in, we get:

$$
\begin{split}
Var(\hat\beta_1|x) & = \sum\limits_{i=1}^n w_i^2 \sigma_i^2 \\
& = \sum\limits_{i=1}^n \left(\frac{x_i - \bar x}{SST_x} \right)^2 \sigma^2_i \\
& = \frac{\sum_{i=1}^n (x_i - \bar x)^2 \sigma^2_i}{SST_x^2}
\end{split}
$$

Of course, just like with homoscedasticity, we do not know $\sigma_i^2$, and have to estimate it with $\hat u$. We do not need a degrees of freedom adjustment in this case. Thus, our estimate of variance is:

$$
\widehat{Var}(\hat\beta_1|x) = \frac{\sum_{i=1}^n (x_i - \bar x)^2 \hat u^2_i}{SST_x^2}
$$

And thus, the standard error (square root of variance) of our estimate $\hat\beta_1$ under homoscedasticity is the square root of our estimate of variance.
:::

The robust standard errors for multiple linear regressions are:

$$
\widehat{se}(\hat\beta_j) = \sqrt{\frac{\sum_{i=1}^n \widetilde{r_{ji}}^2 \hat u^2_i}{\sum_{i=1}^n\widetilde{r_{ji}}^2}}
$$

::: {.callout-tip collapse="true" appearance="simple"}
## Deriving Robust Standard Errors for Multiple Linear Regression

Without MLR.5 Homoscedasticity, variance $Var(u|x)$ is no longer constant at $\sigma^2$. Instead, $Var(u|x)$ varies depending on the value of $x$. Let us define the variance as:

$$
Var(u_i|x_{1i}, \dots, x_{ki}) = \sigma_i^2
$$

Let us find the variance of OLS estimates (we will use $\hat\beta_1$ for simplicity, but this applies to any other coefficient $\hat\beta_1 , \dots, \hat\beta_k$. In proving unbiasedness of OLS, we got to this stage:

$$
\hat\beta_1 = \beta_1 + \frac{\sum_{i=1}^n \widetilde{r_{1i}} u_i}{\sum_{i=1}^n \widetilde{r_{1i}}^2}
$$

We know that $\beta_1$ is a constant (the true value in the population), so that never changes. Thus, it cannot be the variance in $\hat\beta_1$. Thus, the second part is the variance in $\hat\beta_1$.

Let us define $w_i$ as following, as a function of $x_1, \dots, x_k$:

$$
w_i = \frac{\widetilde{r_{1i}}}{\sum_{i=1}^n\widetilde{r_{1i}}^2}
$$

This allows us to write $\hat\beta_1$ as:

$$
\hat\beta_1 = \beta_1 + \sum\limits_{i=1}^nw_i u_i
$$

Thus, we can proceed in the same way as the simple linear regression case.

$$
\begin{split}
Var(\hat\beta_1|x_1, \dots, x_k) & = Var\left( \sum\limits_{i=1}^nw_i u_i \biggr|x_1, \dots ,x_k \right) \\ & = \sum\limits_{i=1}^n Var(w_i u_i | x_1, \dots, x_k) \\
& = \sum\limits_{i=1}^n w_i^2 \ Var(u_i | x_1, \dots, x_k) \\
& = \sum\limits_{i=1}^n w_i^2 \sigma^2_i
\end{split}
$$

Now, plugging back in $w_i$, we get:

$$
\begin{split}
Var(\hat\beta_1|x) & = \sum\limits_{i=1}^n w_i^2 \sigma_i^2 \\
& = \sum\limits_{i=1}^n \left(\frac{\widetilde{r_{1i}}}{\sum_{i=1}^n\widetilde{r_{1i}}^2} \right)^2 \sigma^2_i \\
& = \frac{\sum_{i=1}^n \widetilde{r_{1i}}^2 \sigma^2_i}{\sum_{i=1}^n\widetilde{r_{1i}}^2}
\end{split}
$$

Of course, just like before, we do not know $\sigma_i^2$, and have to estimate it with $\hat u$. Thus, our estimate of variance is:

$$
\widehat{Var}(\hat\beta_1) = \frac{\sum_{i=1}^n \widetilde{r_{1i}}^2 \hat u_i^2}{\sum_{i=1}^n\widetilde{r_{1i}}^2}
$$

And thus, the standard error (square root of variance) of our estimate $\hat\beta_1$ under homoscedasticity is the square root of our estimate of variance. This can be generalised to any other coefficient $\hat\beta_1, \dots, \hat\beta_k$.
:::

<br />

<br />

------------------------------------------------------------------------

# **Further Properties of OLS**

### OLS as a Conditional Expectation Function

A **conditional expectation function** (CEF) says that the value of $E(y)$ depends on the value of $x$. We notate a conditional expectation function as $E(y|x)$. As we noted earlier, the linear regression model can be a conditional expectation function of $E(y|x)$.

A **best linear approximation** of a conditional expectation function can take the following form:

$$
E(y_i|x_i) = b_0 + b_1x_i
$$

With parameters $b_0, b_1$ that minimise the mean squared errors (MSE).

$$
\begin{split}
MSE & = E(y_i - E(y_i|x_i))^2 \\
& = \frac{1}{n}\sum\limits_{i=1}^n( y_i - E(y_i|x_i))^2
\end{split}
$$

OLS also best-approximates the conditional expectation function.

::: {.callout-tip collapse="true" appearance="simple"}
## Proof that OLS is the Best Approximation of the CEF

What we want to prove is that the OLS estimates $\hat\beta_0$ and $\hat\beta_1$ best estimate the parameters $b_0$ and $b_1$ of the Conditional Expectation Function, which means that if true, OLS is the best linear approximation of the conditional expectation function.

Suppose we have the conditional expectation function:

$$
E(y_i|x_i) = b_0 + b_1x_i
$$

We also know that our typical regression equation is:

$$
y_i = \beta_0 + \beta_1 x_i + u_i
$$

We know that $E(u_i|x_i) = 0$. Let us define $u_i$ as the following:

$$
u_i = y_i - E(y_i|x_i)
$$

If the above defined $u_i$ is true, $E(u_i|x_i)$ should also be equal to 0. So, let us plug in the above $u_i$ into $E(u_i | x_i)$.

$$
\begin{split}
E(u_i|x_i) & = E(y_i - E(y_i|x_i) \ | \ x_i) \\
& = E(y_i|x_i) - E(y_i|x_i) \\
& = 0
\end{split}
$$

Thus, we know $u_i = y_i - E(y_i|x_i)$ to be true. Thus, rearranging, we know:

$$
y_i = E(y_i|x_i) + u_i
$$

We also know that $y_i = \beta_0 + \beta_1 x_i + u_i$. Thus, the following is true:

$$
\begin{split}
E(y_i|x_i) + u_i & = \beta_0 + \beta_1 + u_i \\
b_0 + b_1 + u_i & = \beta_0 + \beta_1 + u_i \\
b_0 + b_1 & = \beta_0 + \beta_1
\end{split}
$$

Well, you might point out, it is still possible that $b_1 ≠ \beta_1$ in this scenario. We can go further. We know that the conditional expectation function minimises the mean of squared errors.

$$
\begin{split}
MSE & = \min\limits_{b_0, b_1} E(y_i - E(y_i|x_i))^2 \\
& = \min\limits_{b_0, b_1} E(y_i - \beta_0 - \beta_1x_i)^2
\end{split}
$$

The first order conditions are (using chain rule and partial derivatives, just like in the OLS derivation):

$$
\begin{split}
& E(y_i - b_0 - b_1x_i) = 0 \\
& E(x_i(y_i - b_0 - b_1x_i) = 0
\end{split}
$$

<br />

Now, recall our OLS minimisation conditions:

$$
\begin{split}
& \sum\limits_{i=1}^n (y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i) = 0 \\
& \sum\limits_{i=1}^n x_i (y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i) = 0
\end{split}
$$

Since by definition, average/expectation is $E(x) = \frac{1}{n} \sum x_i$, we can rewrite the OLS minimisation conditions as:

$$
\begin{split}
& n \times E(y_i - \hat\beta_0 - \hat\beta_1x_i) = 0 \\
& n \times E(x_i(y_i - \hat\beta_0 - \hat\beta_1x_i)) = 0
\end{split}
$$

And since anything multiplied to a zero turns into zero, we can ignore the $n$ in the first order condition, and only focus on the expected value part. Thus, our conditions are:

$$
\begin{split}
& E(y_i - \hat\beta_0 - \hat\beta_1x_i) = 0 \\
& E(x_i(y_i - \hat\beta_0 - \hat\beta_1x_i)) = 0
\end{split}
$$

Which as we can see, are the exact same minimisation conditions as the conditional expectation function. Thus, OLS is the best approximation of the conditional expectation function.
:::

This property is very useful for causal inference, as it means OLS is calculated the expected $y$, which allows us to find causal effects by comparing the expected $y$ of the treatment and control groups (assuming the OLS estimator is unbiased).

<br />

### OLS as a Difference-in-Means Estimator

One of the most common estimators is the difference-in-means estimator, which finds the expected value of $y$ in two different categories, then finds the difference in expected value of the two categories.

-   This is frequently used in randomised controlled trials.

When the explanatory variable is binary, a regression estimated with OLS becomes a difference-in-means estimator, with $\widehat{\beta_1}$ being the difference in sample means between catgories $x=1$ and $x=0$.

::: {.callout-tip collapse="true" appearance="simple"}
## Proof OLS is Difference-in-Means Estimator

Previously, we proved that OLS is the best approximation of the conditional expectation function $E(y|x)$.

We can use this property to quickly prove that OLS is a difference-in-means estimator. Assume $x$ is a binary variable with two categories, $x=0$ and $x=1$. We can find the expected value of $y$ for these two categories of $x$:

$$
\begin{split}
E(y_i | x_i = 0) & = \beta_0 + \beta_1(0) = \beta_0 \\
E(y_i | x_i = 1) & = \beta_0 + \beta_1(0) = \beta_0 + \beta_1
\end{split}
$$

Now, let us find the difference in means $E(y)$ for categories $x = 1$ and $x=0$:

$$
E(y_i | x_i = 1) - E(y_i | x_i = 0) = \beta_0 + \beta_1 - \beta_0 = \beta_1
$$

Thus, the OLS estimate of $\widehat{\beta_1}$ is thus the difference in means $E(y)$ of categories $x = 1$ and $x = 0$.

You can also prove this using the OLS estimator formula, and doing complex summation algebra, however, it is easier to prove it this way (and just as effective).
:::

<br />

### OLS as a Method of Moments Estimator

The Method of Moments Estimator is another estimator of the true value of populations in the parameter. The estimator defines key population **moments** of interest - which are the population parameters written in terms of expected value functions set equal to 0.

Then, the Method of Moments uses the sample equivalents of the population moments to estimate the population parameter.

-   For example, to estimate the population mean, the Method of Moments uses the sample mean.

::: {.callout-tip collapse="true" appearance="simple"}
## Details of the Method of Moments Estimator

In order to define a method of moments for a set of parameters $\theta_1, \dots, \theta_k$, we need to specify at least one population moment per parameter. Or in other words, we must have more than $k$ population moments.

Our population moments can be defined as the expected value of some function $m(\theta; y)$ that consists of both the variable $y$ and our unknown parameter $\theta$. The expectation of the function $m(\theta; y)$ should equal 0.

$$
E(m(\theta; y)) = 0
$$

Our sample moments will be the sample analogues of $\theta$ and $y$, which are $\hat\theta$ and $y_i$:

$$
\frac{1}{n}\sum\limits_{i=1}^n m(\hat\theta; y_i) = 0
$$

-   The $\frac{1}{n} \sum$ is there because the definition of expectation/mean is that.
:::

::: {.callout-tip collapse="true" appearance="simple"}
## Example of the Method of Moments Estimator

Let us say that we have some random variable $y$, with a true population mean $\mu$. We want to estimate $\mu$, but we only have a sample of the population.

How can we define our true population parameter $\mu$ in an expectation equation of the form: $E(m(\mu, y)) = 0$?

-   Well, what is $\mu$, the mean, intuitively speaking? It is the expectation of $y$, so $\mu = E(y)$.

Now that we know that $\mu = E(y)$, since they are equal, $\mu - E(y) = 0$. Thus, we can define the mean as a moment of the following condition:

$$
E(y - \mu) = 0
$$

The method of moments says we should use the sample equivalent of the population parameter. The sample equivalent of $\mu$ (the true mean of the population), is of course, the sample mean $\bar y$.

Thus, our sample estimate of the moment would be:

$$
E(y_i - \hat\mu) = \frac{1}{n}\sum\limits_{i=1}^n (y_i - \hat\mu) = 0
$$

With this equation, we can then solve for $\hat\mu$:

$$
\begin{split}
0 & = \frac{1}{n}\sum\limits_{i=1}^n (y_i - \hat\mu) \\
0 & = \frac{1}{n}\sum\limits_{i=1}^ny_i - \frac{1}{n}\sum\limits_{i=1}^n \hat\mu \\
0 & = \frac{1}{n}\sum\limits_{i=1}^ny_i - \frac{1}{n} n \hat\mu\\
0 & = \bar y - \hat \mu \\
\hat\mu & = \bar y
\end{split}
$$

So, we see the method of moments estimates our true population mean $\mu$, with the sample mean $\bar y$.
:::

OLS is a special case of the Method of Moments Estimator. This property is useful as the Instrumental Variables Design is built on this idea.

::: {.callout-tip collapse="true" appearance="simple"}
## Proof OLS is a Method of Moments Estimator

Consider the bivariate regression model:

$$
y = \beta_0 + \beta_1x + u
$$

The OLS estimator can be derived as a method of moments estimator, with 2 moments (expectation functions set equal to 0), one for each parameter ($\beta_0, \beta_1$):

$$
\begin{split}
& E(y-\beta_0 -\beta_1x) = 0 \\
& E(x(y - \beta_0 - \beta_1 x)) = 0
\end{split}
$$

Since we know $u = y - \beta_0 - \beta_1 x$, we can rewrite the two moments as:

$$
\begin{split}
& E(u) = 0 \\
& E(xu) = 0
\end{split}
$$

The estimates of these moments would use the sample equivalents: $\hat\beta_0$ and $\hat\beta_1$.

$$
\begin{split}
& E(y-\hat\beta_0 -\hat\beta_1x) = 0 \\
& E(x(y - \hat\beta_0 - \hat\beta_1 x)) = 0
\end{split}
$$

<br />

Let us prove that OLS is a special case of the method of moments estimator. Remember our OLS minimisation conditions:

$$
\begin{split}
& \sum\limits_{i=1}^n (y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i) = 0 \\
& \sum\limits_{i=1}^n x_i (y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i) = 0
\end{split}
$$

Since by definition, average/expectation is $E(x) = \frac{1}{n} \sum x_i$, we can rewrite the OLS minimisation conditions as:

$$
\begin{split}
& n \times E(y_i - \hat\beta_0 - \hat\beta_1x_i) = 0 \\
& n \times E(x_i(y_i - \hat\beta_0 - \hat\beta_1x_i)) = 0
\end{split}
$$

And since anything multiplied to a zero turns into zero, we can ignore the $n$ in the first order condition, and only focus on the expected value part. Thus, our conditions are:

$$
\begin{split}
& E(y_i - \hat\beta_0 - \hat\beta_1x_i) = 0 \\
& E(x_i(y_i - \hat\beta_0 - \hat\beta_1x_i)) = 0
\end{split}
$$

Which as we can see, are the exact same minimisation conditions as the method of moments estimator. Thus, the OLS estimator is a special case of the Method of Moments estimator, and they produce the same coefficients.
:::

<br />

### OLS as a Maximum Likelihood Estimator

The Maximum Likelihood Estimator is an estimation procedure that is used for Logistic Regression and Count Regression (and many other statistical concepts). Refer to the regression notes to see the details of the maximum likelihood estimator.

OLS is a special case of the maximum likelihood estimator under two conditions:

1.  The error term $u$ must be normally distributed, such that $n \sim \mathcal N(0, \sigma^2)$.
2.  We assume homoscedasticity: $Var(u|x) = \sigma^2$.

::: {.callout-tip collapse="true" appearance="simple"}
### Proof of OLS and MLE Equivalency

A normal distribution has the following probability density function:

$$
f(y) = \frac{1}{\sqrt{2 \pi \sigma^2}}e^{\left( \frac{(x- \mu)^2}{2 -\sigma^2}\right)}
$$

Assuming our error term is normally distributed, we know that the conditional probability density function of our linear model $y = \beta_0 + \beta_1 x + u_i$ is as follows:

$$
f(y_i|x_i;\beta_0, \beta_1, \sigma^2) = \frac{1}{\sqrt{2 \pi \sigma^2}}
 e^{ \left( -\frac{1}{2 \sigma^2}(y_i - \beta_0 - \beta_1 x_i)^2 \right)}$$

By the independence of $y_1, \dots, y_n$, the likelihood function is:

$$
\begin{split}
L & = \prod_{i=1}^nf(y_i|x_i; \beta_0, \beta_1, \sigma^2) \\
& =  \frac{1}{\sqrt{2 \pi \sigma^2}}
 e^{ \left( -\frac{1}{2 \sigma^2}\sum_{i=1}^n(y_i - \beta_0 - \beta_1 x_i)^2 \right)}
\end{split}
$$

The log-likelihood function is thus:

$$
\log L = -\frac{n}{2} \log (2 \pi) - \frac{n}{2} \log (\sigma^2) -\frac{1}{2 \sigma^2}\sum_{i=1}^n(y_i - \beta_0 - \beta_1 x_i)^2
$$

We can maximise the log-likelihood function (and thus the likelihood function) by finding the first order conditions:

$$
\begin{split}
\frac{\partial \log L}{\partial\beta_0} : & \ -\frac{1}{\sigma^2} \sum\limits_{i=1}^n(y_i - \hat\beta_0 - \hat\beta_1 x_i) \times -1 =0 \\
\frac{\partial \log L}{\partial\beta_1} : & \ -\frac{1}{\sigma^2} \sum\limits_{i=1}^n(y_i - \hat\beta_0 - \hat\beta_1 x_i) \times -x_i =0 \\
\frac{\partial \log L}{\partial\sigma^2} : & \ -\frac{n}{2\sigma^2}  + \frac{1}{2\sigma^4} \sum\limits_{i=1}^n(y_i - \hat\beta_0 - \hat\beta_1 x_i) =0 \\
\end{split}
$$

We can focus on the first two conditions, since we are interested in the intercept $\beta_0$ and coefficient $\beta_1$ estimates. We can ignore the initial $\frac{1}{\sigma^2}$, since if the summation equals zero, the whole partial derivative will also equal zero.

Thus, our first order conditions for maximum likelihood estimation are:

$$
\begin{split}
& \sum\limits_{i=1}^n(y_i - \hat\beta_0 - \hat\beta_1 x_i) = 0 \\
& \sum\limits_{i=1}^n(y_i - \hat\beta_0 - \hat\beta_1 x_i)x_i = 0
\end{split}
$$

These conditions are identical to our OLS conditions. Thus, the maximum likelihood estimator is equivalent to OLS given normality of the error term and homoscedasticity.
:::
