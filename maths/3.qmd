---
title: "Linear Algebra"
subtitle: "Kevin's PSPE Notes: Calculus and Linear Algebra"
format:
    html:
        page-layout: article
        grid:
          sidebar-width: 350px
          body-width: 700px
          margin-width: 250px
          gutter-width: 2em
        fontsize: 12pt
        theme: default
        toc: TRUE
        toc-depth: 3
        toc-location: left
        toc-expand: true
        toc-title: "Table of Contents"
        mainfont: "Computer Modern"
        title-block-banner: true
        classoption: fleqn
        html-math-method: katex
editor: visual
---

Use the sidebar for easy navigation.

------------------------------------------------------------------------

# **Vector Spaces**

### Linear Mapping

A mapping is any rule that maps elements from one set to another. A function $f$ is a mapping $f: A \rightarrow B$.

A linear mapping is a mapping that is linear, which must meet the following properties:

-   $f(a+b) = f(a) + f(b)$
-   $f(ca) = c f(a)$

We can represent linear mappings for finite sets by matrices. Let us say $\mathbf X_{n \times m}$ is a matrix, and $\mathbf y_{m}$ is a vector. Let us find their product:

$$
\mathbf {Xy} = \mathbf z_m
$$

What this is saying is take the vector $\mathbf y$, and operate on it using the matrix $\mathbf X$, to produce a new vector $\mathbf z$. Here, the matrix $\mathbf X$ is an operator that maps $\mathbf y \rightarrow \mathbf z$.

<br />

### Vectors and Vector Spaces

Vectors have a norm (which is also called the inner product):

$$
||\mathbf a|| = \sqrt{\mathbf a \cdot \mathbf a} = \sqrt{a_1^2 + a_2^2+\dots+a_n^2}
$$

Vector spaces are any collection of vectors that have a norm, that have some common properties.

Vectors are important because our data is represented in vectors.

<br />

### Linear Combination

A linear combination is a combination of vectors that is linear (i.e. vectors can be added, and scalar multiplied). For example, this is a linear combination:

$$
t \mathbf x + (1-t) \mathbf y
$$

Linear combinations either represents lines (in $\mathbb R^2$), planes (in $\mathbb R^3$), and hyperplanes (a plane of one less dimensions than the space) in higher dimensions.

<br />

### Linear Independence

Start off with a linear combination:

$$
a_1 \mathbf x_1 + a_2 \mathbf x_2+\dots + a_n \mathbf x_n
$$

This set of vectors $(\mathbf x_1, \dots, \mathbf x_n)$ is independent if you cannot get one vector $\mathbf x_j \in \{ \mathbf x_1, \dots, \mathbf x_n \}$ equal to a combination or multiple of the other vectors.

::: {.callout-note collapse="true" appearance="simple"}
## Example of Linear Independence

Let us say we have these two vectors:

$$
\begin{pmatrix} 1 \\ 2 \end{pmatrix}, \begin{pmatrix} 2 \\ 1 \end{pmatrix}
$$

Are these linearly independent? That means I cannot use a linear transformation to go from one to another.

No, there is no constant you can multiply to get from vector 1 to vector 2, and there are no other vectors to add/subtract to to go from one to another.

Now consider these two vectors:

$$
\begin{pmatrix} 1 \\ 2 \end{pmatrix}, \begin{pmatrix} 2 \\ 4 \end{pmatrix}
$$

These are not independent - you can multiply the first vector by a scalar of 2 to get the second vector.
:::

This can be complicated to see in higher dimensions (since it is hard to consider how multiple vectors can be combined to match another). There, we use the matrix rank (see below).

<br />

### Spanning Vectors and Dimension

A collection of spanning vectors spans some space, if you can write any vector in that space, as a linear combination of the spanning vectors.

For example, take vector $\mathbf e_1 = (1 \ 0)$ and $\mathbf e_2 = (0 \ 1)$. These vectors span some space including $\mathbf z$, if vector $\mathbf z$ can be written as:

$$
\mathbf z = a \mathbf e_1 + b \mathbf e_2, \quad \text{e.x.} \quad \mathbf z = (a \ b)
$$

-   In fact, $\mathbf e_1$ and $\mathbf e_2$ span the set of all 2-dimensional vectors.

This allows us to write vectors in terms of the core vectors, and to understand the dimension of the space.

The **dimension** of the space is the smallest number of linearly independent vectors that span the space.

::: {.callout-note collapse="true" appearance="simple"}
## Example of Dimensionality

Take these two vectors:

$$
\begin{pmatrix} 1 \\ 0 \end{pmatrix}, \begin{pmatrix} 0 \\ 1 \end{pmatrix}
$$

These are linearly independent - no factor multipled can get the other vector. Thus, this is 2 dimensional space

Now, let us add a third vector.

$$
\begin{pmatrix} 1 \\ 0 \end{pmatrix}, \begin{pmatrix} 0 \\ 1 \end{pmatrix}, \begin{pmatrix} 3 \\ 2 \end{pmatrix}
$$

Is the third vector linearly independent? No. We can write the third vector with a combination of the other two:

$$
\begin{pmatrix} 3 \\ 2 \end{pmatrix} = 3 \begin{pmatrix} 1 \\ 0 \end{pmatrix} + 2 \begin{pmatrix} 0 \\ 1 \end{pmatrix}
$$

Thus, the third vector is in the space spanned by the first 2 vectors. The first two vectors spans this space, and thus, it is 2 dimensional space.
:::

Generally, the dimensionality of the space matches the number of vectors that span the space (so a 2 dimensional space is often spanned by 2 vectors, 3 spanned by 3, etc.).

Note: Dimensionality of a vector space is not always the same as the dimensionality of the vectors.

<br />

### Matrix Rank

As we discussed before, it can be difficult to find if vectors are linearly independent in higher dimensions.

We can stack the row vectors into a matrix (we can also do them in columns):

$$
\begin{pmatrix} \mathbf x_1 \\ \mathbf x_2 \\ \mathbf x_3 \end{pmatrix} =
\begin{pmatrix} x_{11} & x_{12} & x_{13} \\
x_{21} & x_{22} & x_{23} \\
x_{31} & x_{32} & x_{33} \end{pmatrix}
$$

The **Rank** of a matrix is the number of linearly independent rows/columns in a matrix.

If the Rank is equal to the total number of rows/columns (all rows/columns are linearly independent), the matrix has **full rank**.

-   A matrix with full rank is non-singular, and thus, can be inverted, and has a non-zero determinant.

Thus, if we find the determinant of the matrix, if it is 0, the vectors are [not]{.underline} linearly independent, and if it is not0 , they are linearly independent.

We can also know a matrix is not full rank, if the space of the dimension is less than the number of vectors (as explained above).

<br />

### Quadratic Forms

A quadratic form takes the following form:

$$
\mathbf x^T \mathbf A \mathbf x
$$

-   Where matrix $\mathbf A$ is a square matrix.

If $\mathbf x^T \mathbf A \mathbf x > 0$ for all values, then the matrix is positive definite. If $\mathbf x^T \mathbf A \mathbf x < 0$, then negative definite. If $\mathbf x^T \mathbf A \mathbf x â‰¤ 0$ it is positive semi-definite.

This will be useful for optimisation.

<br />

<br />

------------------------------------------------------------------------

# **Solving Systems of Equations**

### Matrices and Systems of Equations

You can write any system of linear equations as a matrix times a vector. Let us take this set of equations:

$$
\begin{cases}
a_{11}x_1 + a_{12} x_2 + a_{13} x_3 = y_1 \\
a_{21}x_1 + a_{22}x_2 + a_{23}x_3 = y_2 \\
a_{31}x_1 + a_{32}x_2 + a_{33}x_3 = y_3
\end{cases}
$$

We can write this system of equations as follows:

$$
\begin{pmatrix}
a_{11} & a_{12} & a_{13} \\
a_{21} & a_{22} & a_{23} \\
a_{31} & a_{32} & a_{33}
\end{pmatrix} \begin{pmatrix} x_1 \\ x_2 \\ x_3 \end{pmatrix}
 = \begin{pmatrix} y_1 \\ y_2 \\ y_3 \end{pmatrix}
$$

Or even simpler, we can represent it as:

$$
\mathbf A\mathbf x = \mathbf y
$$

A unique solution exists when you have the same amount of equations as unknowns, and the equations are non-contradictory.

-   Overdetermined systems are when there are more equations than unknowns, which might contradict each other.
-   Underdetermined systems are when there are not enough equations compared to unknowns, so we cannot solve it.

<br />

### Matrix Rank and Systems of Equations

Take this system of linear equations:

$$
\begin{cases}
x+y=1 \\
2x + 2y = 2
\end{cases}
$$

We can see that the two equations are a common factor of each other. Or in other words, these two vectors are non-linearly independent.

We know when solving for these equations, we cannot actually solve for a unique solution.

We know if a matrix is full rank, then all rows/columns are linearly independent.

<br />

### Matrix Inversion

Matrix inversion is a way to solve a system of equations. Take this system of equations:

$$
\mathbf{Ax} = \mathbf y
$$

You can solve for $\mathbb x$ by inverting matrix $\mathbf A$ (assuming matrix a is full rank):

$$
\begin{split}
\mathbf{Ax} & = \mathbf y \\
\mathbf{A}^{-1}\mathbf{Ax} & = \mathbf A^{-1}\mathbf y \\
\mathbf x &= \mathbf A^{-1}\mathbf y
\end{split}
$$

::: {.callout-note collapse="true" appearance="simple"}
## Example of Matrix Inversion

Take this system of equations:

$$
\begin{cases}
3x - 7y = -11 \\
5x + 10y = 25
\end{cases}
$$

We can write this in linear algebra:

$$
\begin{pmatrix}
3 & -7 \\
5 & 10
\end{pmatrix} \begin{pmatrix} x \\ y \end{pmatrix} =
\begin{pmatrix} -11 \\ 25 \end{pmatrix}
$$

Now, let us find the inverse of the first matrix:

$$
\mathbf A^{-1} = \frac{1}{|\mathbf A|}\mathbf C^T
$$

We know the determinant $|\mathbf A| = 3(10) - (-7)(5) = 65$.

Now, let us find the cofactor matrix:

-   $c_{11} = (-1)^{1+1}\times 10 = 10$
-   $c_{12} = (-1)^{1+2} \times 5 = -5$
-   $c_{21} = (-1)^{2+1} \times -7 = 7$
-   $c_{22} = (-1)^{2+2} \times 3 = 3$

Thus, the cofactor matrix transposed should be:

$$
\mathbf C^T = \begin{pmatrix} 10 & -5 \\ 7 & 3 \end{pmatrix}^T = \begin{pmatrix} 10 & 7 \\ -5 & 3 \end{pmatrix}
$$

Thus, the inverse of matrix $\mathbf{A}$ should be:

$$
\mathbf A^{-1} = \frac{1}{65} \begin{pmatrix} 10 & 7 \\ -5 & 3 \end{pmatrix} = \begin{pmatrix} \frac{2}{13} & \frac{7}{65} \\ -\frac{1}{13} & \frac{3}{65} \end{pmatrix}
$$

We know the solution should be:

$$
\begin{pmatrix} x \\ y \end{pmatrix} = \begin{pmatrix} \frac{2}{13} & \frac{7}{65} \\ -\frac{1}{13} & \frac{3}{65} \end{pmatrix} \begin{pmatrix} -11 \\ 25 \end{pmatrix}
$$

Thus, doing matrix multiplication to obtain $x$ and $y$:

-   $x =\frac{2}{13}(-11) + \frac{7}{65}(25) = -\frac{22}{13} + \frac{35}{13} = \frac{13}{13}=1$
-   $y=-\frac{1}{13}(-11) + \frac{3}{65}(25) = \frac{11}{13}+ \frac{15}{13} = \frac{26}{13} = 2$

The solution to this system of equations is: $(1, 2)$.
:::

<br />

### Cramer's Rule

Cramer's rule is another rule to solve equations, only for square matrices. It is not super commonly used.

Given the system of equations:

$$
\mathbf{Ax} = \mathbf y
$$

The element $x_i$ is defined as:

$$
x_i = \frac{|\mathbf B_i|}{\mathbf A}
$$

Where $\mathbf B_i$ is a matrix obtained by taking the matrix $\mathbf A$, and replacing the $i$th column with the column vector $\mathbf y$.

<br />
