---
title: "Counting, Probability, Random Variables"
subtitle: "Session 4, GV4L8"
output:
  html_document:
    df_print: paged
    toc: yes
    toc_float:
      collapsed: false
    toc_depth: 4
    theme: lumen
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{=html}
<style>
  body .main-container {
    max-width: 1100px;
    font-size: 12pt;
  }
</style>
```
[GV4L8 Homepage](https://kevinli03.github.io/notes/#GV4L8_Introductory_Maths)

------------------------------------------------------------------------

# Counting

### Fundamental Theorem of Counting

The fundamental theorem of counting determines the number of possible outcomes, when there are multiple stages or events:

-   If event $A$ can occur in $m$ ways, and event $B$ can occur in $n$ ways - then the amount of ways that both $A$ and $B$ can possibly occur is $m \times n$

Basically, the total amount of ways 2 events can occur - is the amount of ways 1 can occur multiplied by the amount of ways the other can occur

-   This also applies to more events

<br />

For example, if we have a 3 character password, where the 1st digit is a letter (26 choices), the second is a number between 0 and 9 (10 choices), and the third is a symbol (5 choices), how many different passwords can we create?

-   Simply, multiply $26 \times 10 \times 5 = 1300$

<br />

### Factorials

Factorials are a mathematical operation, that is commonly used in counting.

Factorials are defined as:

$$
n! = n \times (n-1) \times (n-2) \times ... \times 1
$$

For example, $5!$ would be:

$$
5! = 5 \times 4 \times 3 \times 2 \times 1 = 120
$$

<br />

Factorials are useful when discussing arrangements (where order matters - a different order is a different arrangement)

-   Ex. How many ways can we re-arrange ABC?

-   ABC, ACB, BAC, BCA, CAB, CBA

-   We can explore this mathematically - for the first position, we can choose either A, B, or C (3 choices). For the second position, we only have 2 (because we have already chosen one). Then, for the final position, we only have 1 (because the other 2 have been chosen)

-   Thus, the number of arrangements is $3 \times 2 \times 1 = 3!$

<br />

### Permutations

Permutations are not super common in politics, but they do exist.

Permutations is the arrangement of objects, when order is important:

-   For example, ABC, and CBA, are considered different arrangements because the order is different.

<br />

The number of arrangements when order matters, when you take $r$ number of objects from $n$ total objects, is as follows:

$$
P(n,r) = \frac{n!}{(n-r)!}
$$

<br />

For example, what is the number of ways to arrange 3 books out of 5 from a shelf?

-   For the first book, we can choose from 5 books on the shelf (5 options)

-   For the second book, since we already took a book off, we only have 4 options left

-   For the third book, since we already took 2 books off, we only have 3 options left.

-   We stop there, because we only want to select 3 books.

-   Thus, the probability is $5 \times 4 \times 3$

Notice, this is the same as the permutation formula result:

$$
P(5,3) = \frac{5!}{(5-3)!} = \frac{5 \times 4 \times 3 \times 2 \times 1}{2 \times 1} = 5 \times 4 \times 3
$$

<br />

### Combinations

Combinations differ from permutations, because order is not important

-   For example, ABC and CBA are considered 1 outcome, because the order doesn't matter - only what objects are included in them

-   This means combinations will yield less results than permutations (simply because we don't count different orders)

<br />

The number of arrangements when order does not matter, when you take $r$ number of objects from $n$ total objects, is as follows:

$$
C(n,r) = \frac{n!}{r!(n-r)!}
$$

<br />

Lets take the same example as above, selecting 3 books from 5 on a shelf. But this time, order does not matter:

-   We will notice that combinations have an extra $r!$ in the denominator. Why?

-   Well, $r$ is the number of books we are taking off the shelf, and thus, $r!$ is the number of arrangements of those books.

-   We don't care about the arrangement of those books in a combination

-   Thus, we do an extra division of $r!$

Thus, the number of possibilities is:

$$
C(5,3) = \frac{5!}{3!(5-3)!} = 10
$$

<br />

------------------------------------------------------------------------

[GV4L8 Homepage](https://kevinli03.github.io/notes/#GV4L8_Introductory_Maths)

# Probability

### Definition of Probability

Probability measures the likelihood of an event occurring

-   Must be between 0 and 1

-   Where 0 means the event for certain will not happen, and 1 means the event is certain to happen

-   More formally, $0 ≤ P(A) ≤ 1$

For example, the probability of heads is 0.5

<br />

A sample space is a space of all the possibilities (including the possibility of nothing happen).

-   It is the sum of all probabilities relating to the event

<!-- -->

-   The area of the sample space is 1 - something has to happen

For example, the sample space of flipping a coin is the space of all possibilities - getting a heads, and getting a tails (assuming it is impossible to land on the side).

-   The total probability of the sample space (sum of all the probabilities) - getting either heads or tails, is 1

<br />

### Axioms of Probability

There are three fundamental "axioms" (truths) in probability theory:

1.  Non-Negativity - the probability cannot be negative
2.  Normalisation - the probability of the entire sample space $S$ is 1
3.  Countable Additivity - if events are mutually exclusive, the probability of their union (both occurring) is the sum of their individual properties
    -   Mutual Exclusivity means if one event happens, the other one cannot happen
    -   Union is the probability of both events occuring $P(A \cup B)$

<br />

Let us give an example of countable additivity: rolling a dice

-   The outcomes are 1,2,3,4,5,6

-   All outcomes are mutually exclusive - if you roll a 2, it cannot be a 3 at the same time

Thus, since they are mutually exclusive, the probability of multiple events occurring is the sum of the probability of the events:

-   For example - the probability of rolling a 1,2, or 3 is $P(1) + P(2) + P(3)$

<br />

### Joint Probability

Joint Probability is the opposite of mutual exclusivity - it is about two or more events occurring simultaneously (at the same time):

-   The probability of $A$ and $B$ occurring together is $P(A \cap B)$

<br />

For example, in a deck of cards, you can draw a red card at the same time as a heart (they are not mutually exclusive).

-   Thus, the joint probability is the probability of getting a card that is both red and heart at the same moment

<br />

### Conditional Probability

Conditional probability is the probability of one event, given another event has already happened

-   Probability of $A$ given $B$ has already occurred is labeled as $P(A|B)$

Conditional Probability is calculated as the following:

$$
P(A|B) = \frac{P(A \cap B)}{P(B)}
$$

<br />

For example, what is the probability of drawing an ace, given you already know you drew a spade.

$$
P(A \cap B) = 1/52
$$

$$
P(B) = 13/52
$$

$$
P(A|B) = \frac{P(A \cap B)}{P(B)} = \frac{1/52}{13/52} = \frac{1}{13}
$$

<br />

### Law of Total Probability

Law of Total Probability helps calculate the probability of an event, by considering different mutually exclusive events that partition the sample space.

For example, the probability of being a smoker, by considering different groups (males and females) with different smoking rates:

-   Male and Female are (traditionally) mutually exclusive events

-   Probability of smoking as a male is a conditional probability (see above), same as probability of smoking as a female

Thus, the law of total probability states that knowing these conditional probabilities, given they are mutually exclusive, we know the total probability of smoking is:

$$
P(A) = \sum P(A|B_i) \times P(B_i)
$$

$$
=P(A|B_M) \times P(B_M) + P(A|B_F) \times P(B_F)
$$

Where $B_M$ is male and $B_F$ is female.

<br />

For this law to apply, the following conditions must be met:

-   The $B_i$ events are mutually exclusive (ex. male and female are mutually exclusive)

-   All the $B_i$ events consist of the entire sample space (ex. male and female [at least traditionally] consist of 100% of the population)

<br />

Another Example: Imagine you only have 3 routes you can get to work.

-   $P(A)$ is the probability you arrive on time

-   $P(B_1)$ is the probability you take route 1 (same goes for routes 2,3)

-   $P(A|B_1)$ is the probability you are on time, given you take route 1 (and same goes for routes 2,3)

Thus, the total probability of arriving on time $P(A)$, by the law of total probability, is:

$$
P(A) = P(A|B_1)P(B_1) + P(A|B_2)P(B_2) + P(A|B_3) P(B_3)
$$

<br />

### Bayes' Theorem

Bayes' Theorem provides a way to "update" the probability of an event based on new evidence.

-   Relates the conditional probability of $A$ given $B$, to the conditional probability of $B$ given $A$

Bayes Theorem is one of the most important things to remember about statistics, and is very useful

-   For example - you can use it to update the probability of having a disease, after you have received a positive test result (remember, it is not 100%, tests can be wrong)

<br />

Bayes Theorem states the following relationship:

$$
P(A|B) = \frac{P(B|A) \times P(A)}{P(B)}
$$

Where:

-   $P(A|B)$ is the posterior probability (probability of $A$ given $B$ has occurred)

-   $P(B|A)$ is the likelihood (probability of $B$ given $A$ has occurred)

-   $P(A)$ is the prior probability (initial probability of $A$ before the new info

-   $P(B)$ is the marginal probability (total probability of $B$)

<br />

### Deriving Bayes' Theorem

We start with the definition of conditional probability (see above):

$$
P(A|B) = \frac{P(A \cap B)}{P(B)}
$$

We can rearrange that equation to get:

$$
P(A \cap B) = P(A|B) \times P(B)
$$

<br />

Then, let us consider the conditional probability of $B|A$ (basically, conditional probability reversed)

$$
P(B|A) = \frac{P(B \cap A)}{P(A)}
$$

Do the same rearrangement as above:

$$
P(B \cap A) = P(B|A) \times P(A)
$$

<br />

We know by the laws of sets, that:

$$
P(A \cap B) = P(B \cap A)
$$

Thus, we can set the other sides of the equations equal to each other:

$$
P(A|B) \times P(B) = P(B|A) \times P(A)
$$

And isolate $P(A|B)$ for Bayes' Theorem:

$$
P(A|B) = \frac{P(B|A) \times P(A)}{P(B)}
$$

<br />

### Boole's Inequality

Boole's Inequality provides an upper bound of the probability of the union of multiple events:

$$
P \left( \bigcup^n_{i=1} A_i \right) ≤ \sum\limits_{i=1}^n P(A_i)
$$

<br />

Basically - the probability of multiple events occurring, is equal or less than the sum of there probabilities. Why?

-   Well, we know that the probability of multiple mutually exclusive events is equal to the sum of the individual probabilities

-   If they are not mutually exclusive, the probability will be less than the sum of the probabilities (because of overlap)

-   Thus, there is no chance where the probability of multiple events is greater than the sum of the individual probabilities

<br />

------------------------------------------------------------------------

[GV4L8 Homepage](https://kevinli03.github.io/notes/#GV4L8_Introductory_Maths)

# Random Variables

### Levels of Measurement

Data is often classified into four levels of measurement:

-   Nominal/Categorical - No meaningful order to categories

-   Ordinal - categories, but they have a order, but not consistent distance

-   Interval: Ordered values with consistent differences between values

-   Ratio: Interval data with meaningful zero

<br />

### Random Variables

A random variable is a numerical outcome of a random process

-   Assigns a real number to each possible outcome

-   Discrete Random Variable: can take on countable values (1, 2, 3, but not decimals) - used for categorical and ordinal measurements

-   Continuous Random Variable: can take on any real number in an interval - used to interval and ratio measurements

Random variables can be graphed into distribution functions based on the probability of each outcome

-   X axis is the outcomes (1,2,3,4 for example)

-   Y axis is the probability of the outcome

<br />

### Probability Mass Functions

The Probability Mass Function gives the probability of some outcome in a discrete random variable

-   For example, if you roll a dice, the outcomes are discrete (1,2,3,4,5, or 6).

-   The Probability Mass Function tells you the probability of getting a certain outcome

<br />

Probability mass functions are notated as following:

$$
P(X=x) = p(x)
$$

Where the following:

-   Big $X$ is the possible outcomes

-   Small $x$ is the outcome we plug in, that we are interested in the probability

<br />

Probability Mass Functions have the following properties:

-   $P(X=x)$ for any $x$ event should be between 0 and 1 (since it is a probability of an outcome, and probability cannot exceed 0 or 1)

-   Sum of all the probabilities for every $x$ is 1, since something has to happen

<br />

### Probability Density Function

The Probability Density Function gives the probability of some range of outcomes in a continuous random variable

-   Not a single point, since the variable is continuous, so it would be weird to find the chance of outcome being, for example, 1.2352

Probability Density Functions are notated as $f(x)$

<br />

Probability of any specific point is zero

-   So, we have to use an interval

Then, we can calculate the probability between $a$ and $b$ as:

$$
\int\limits_a^b f(x)dx
$$

<br />

Also, the total sum of all outputs is 1 (since something has to happen)

$$
\int\limits_{-∞}^∞ f(x) = 1
$$

<br />

### Expectations

Expectations, also called expected value or mean, is the key summary measure

-   Represents the weighted average of all possible values of the variable, with the weights being probability

-   Basically, the best "guess" of the value of a random variable given the probabilities and possible outcomes

<br />

For discrete random variable $Y$, the expected value is:

$$
E(Y) = \sum\limits_i y_i \times p(y_i)
$$

-   Where $y_i$ is some outcome value, and $p(y_i)$ is the probability that specific outcome occurs

For example, tossing the fair die, the expected value is:

$$
E(Y) = \frac{1}{6} (1+2+3+4+5+6) = 3.5
$$

<br />

For a continuous random variable $Y$, the expected value is the centre or average of $Y$

-   A weighted average, where the weights are given by the probability of each outcome (which is given by the probability density function)

It is the same as a mean or average:

$$
\int\limits_{-∞}^{∞} y \times f(y)dy
$$

-   Where $f(y)$ is the probability density function, and $y$ is the outcome value

-   Same intuition as the discrete random variable we showed above.

<br />

### Measures of Spread

The deviation of a value $y$, from the mean $E[Y]$ is: $y - E[Y]$

-   Basically, how far is the value from the mean

The squared deviation is the deviation squared: $(y-E[Y])^2$

-   This gets rid of the negatives

<br />

Then, we get to the **variance**: which measures the spread of a random variable around the mean

-   Essentially, the expected value of the squared deviations

$$
Var(Y) = \sigma^2= E \left[ (y-E[Y])^2 \right]
$$

The **standard deviation** is the square root of the variance:

$$
\sigma = \sqrt{Var(Y)}
$$

Both these present a measure of how "spread out" or "dispersed" your data is

<br />

### Covariance and Correlation

Covariance measures how two random variables vary together:

$$
Cov(X,Y) = E[ \space (X-E(X)) \times (Y-E(Y)) \space ]
$$

$$
=E[XY] - E[X] \times E[Y]
$$

<br />

Correlation is covariance, but normalised

-   Basically, covariance, but weighted by the standard deviation of the variables of interest

-   This tells us the relationship between $X$ and $Y$

$$
= \frac{Cov(X,Y)}{\sigma_x \sigma_y}
$$

This weighted structure means covariance is always between -1 and 1

-   Negative numbers are negative relationship

-   Positive numbers are positive relationship

-   0 is no relationship

<br />

### Normal Distribution

The Normal Distribution, also called Gaussian Distribution, describes a form a random continuous variable distribution

-   Where all 3 measures of central tendency (mean, median, and mode) are equal

-   There is one central peak, and it is symmetric and bell shaped

<br />

The Probability Density Function of a Normal Distribution is as follows:

$$
f(y) = \frac{1}{\sqrt{2 \pi \sigma^2}} e^{ \left (-\frac{(y - \mu) ^2}{2 \sigma^2} \right) }
$$

Where $\sigma^2$ s the variance, and $\mu$ is the mean.

<br />

The normal distribution has a cool rule - the 68-95-99.7 Rule:

-   68% of the data falls within one standard deviation from the mean: $\mu ± \sigma$

-   95% of the data falls within 2 standard deviations from the mean: $\mu ± 2\sigma$

-   99.7% of the data falls within 3 standard deviations from the mean: $\mu ± 3 \sigma$

This allows us to calculate confidence intervals of estimates

<br />

------------------------------------------------------------------------

[GV4L8 Homepage](https://kevinli03.github.io/notes/#GV4L8_Introductory_Maths)
