---
title: "Introduction to Regression Modelling"
output:
  html_document:
    df_print: paged
    toc: yes
    toc_float:
      collapsed: true
    toc_depth: 4
    theme: lumen
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{=html}
<style>
  body .main-container {
    max-width: 1100px;
    font-size: 12pt;
  }
</style>
```
[MY452A Homepage](https://kevinli03.github.io/notes/#MY452A_Regression_Analysis)

Week 1, Applied Regression Analysis

-   Title: Introduction to Regression Modelling

-   Topics: Simple Linear Regression

-   Readings:

    1.  Chapter 1, Kuha, J., and Lauderdale, B. (No date) *Applied Regression Analysis: Coursepack*

    2.  Section 3.1-3.3, Kuha, J., and Lauderdale, B. (No date) *Applied Regression Analysis: Coursepack*

    3.  Chapters 1, Gelman et al (2022) *Regression and Other Stories*

<br />

------------------------------------------------------------------------

[MY452A Homepage](https://kevinli03.github.io/notes/#MY452A_Regression_Analysis)

# Key Points

Here are some [**conventional notation**]{.underline}:

-   Response variable notated as $Y$

    -   On this course, only one $Y$ variable, but there can be more

-   Explanatory variables notated as $X$

    -   There are often more than one $X$, so we have $X_1, X_2, ..., X_k$

    -   Sometimes, to denote all $X$ variables together, we will notate it as a vector $\overrightarrow{X} = (X_1, X_2, ..., X_k)$ (can also be bolded)

<br />

In formal statistical terminology, a regression model is a specification of the conditional distribution of $Y$ given $\overrightarrow{X}$

-   It states that the distribution of values that $Y$ will have, depends on the value of $\overrightarrow{X}$

-   So that sets of observations with different $\overrightarrow{X}$ will have different distributions of $Y$

<br/>

[**Linear Regression Models**]{.underline} take the following form:

$$
E(Y|\overrightarrow{X}) = \alpha + \beta_1 X_1 + \beta_2 X_2 + ... + \beta_k X_k
$$

These are the features of this linear model:

-   $E(Y|\overrightarrow{X})$ is the expected value of the conditional distribution of $Y$ given some $X$

-   $\beta_1, \beta_2, ... \beta_k$ are the regression coefficients - each attached to a $X$ variable.

    -   We sometimes notate these coefficients as a vector $\overrightarrow{\beta}$

-   Coefficient $\beta_j$ of explanatory variable $X_j$ describes the direction and strength of the relationship of $X_j$ and $Y$, holding other $X$ constant

-   $\alpha$ or $\beta_0$ is the constant or intercept term - when all $X = 0$, what is $Y$

<br />

Formally, linear regression requires that $Y$ is a continuous variable (interval or ratio level)

<br />

------------------------------------------------------------------------

[MY452A Homepage](https://kevinli03.github.io/notes/#MY452A_Regression_Analysis)

# Intro to Regression

### What is Regression?

Situations where we have a response value, and a set of explanatory variable:

-   How do the explanatory variables relate to the response variable

-   Methods for predicting values - given explanatory variable values, what would you predict the response to be

Very important - many other methods build on regression

<br />

Regression can help with the following:

-   Describing relationships and associations between variables

-   Estimating causal effects

-   Prediction of new values of the response variable

<br />

### Learning about Regressions

What does it mean to learn about regressions?

1.  Technical: formal structure - definitions, estimation, formal interpretation, implementation
2.  Conceptual: how do we use the technical in actual research design

The focus is on technical aspects, but conceptual questions are always in the background

<br />

For different regression models, we want to learn about:

-   Model specification and building

-   Methods of estimation and inference for parameters

-   Using tools to fit and examine models

-   Interpretation of models

-   Criticism of models

<br />

### Conventional Notation

Here are some conventional notations:

-   Response variable notated as $Y$

    -   On this course, only one $Y$ variable, but there can be more

-   Explanatory variables notated as $X$

    -   There are often more than one $X$, so we have $X_1, X_2, ..., X_k$

    -   Sometimes, to denote all $X$ variables together, we will notate it as a vector $\overrightarrow{X} = (X_1, X_2, ..., X_k)$ (can also be bolded)

<br />

------------------------------------------------------------------------

[MY452A Homepage](https://kevinli03.github.io/notes/#MY452A_Regression_Analysis)

# Regression Models

### Specification of Regression

In formal statistical terminology, a regression model is a specification of the conditional distribution of $Y$ given $\overrightarrow{X}$

-   It states that the distribution of values that $Y$ will have, depends on the value of $\overrightarrow{X}$

-   So that sets of observations with different $\overrightarrow{X}$ will have different distributions of $Y$

<br />

For example, say $X$ is age, $Y$ is attitude towards immigration

-   If $X = 30$, then we are wondering what the conditional distribution of $Y$ given $X=30$ - given a particular age, what is the distribution of their attitudes towards immigration

    -   We focus on the mean of that distribution

-   If $X$ changes (i.e. age changes), the conditional distribution of $Y$ given $X$ also changes

<br />

### Linear Regression Coefficients

We we focus on the linear regression model in the first few weeks:

$$
E(Y|\overrightarrow{X}) = \alpha + \beta_1 X_1 + \beta_2 X_2 + ... + \beta_k X_k
$$

These are the features of this linear model:

-   $E(Y|\overrightarrow{X})$ is the expected value of the conditional distribution of $Y$ given some $X$

-   $\beta_1, \beta_2, ... \beta_k$ are the regression coefficients - each attached to a $X$ variable.

    -   We sometimes notate these coefficients as a vector $\overrightarrow{\beta}$

-   Coefficient $\beta_j$ of explanatory variable $X_j$ describes the direction and strength of the relationship of $X_j$ and $Y$, holding other $X$ constant

-   $\alpha$ or $\beta_0$ is the constant or intercept term - when all $X = 0$, what is $Y$

<br />

### Parameter Estimation

We use observed data on our variables, to learn about a model and its parameters

-   Observations are elements with values of all $X$ variables and $Y$ variable: $(\overrightarrow{X}_i, Y_i)$ for $i = 1,...,n$, where $n$ is the number of observations

We then obtain estimates of the regression coefficients

-   Specific values that best match the observed data

-   This is called "fitting the model" to the data

-   We typically put a "hat" on estimated parameters (ex. $\hat{\beta_1}$)

We also carry out statistical inference (significance tests and confidence intervals) for the coefficients

<br />

### Fitted Values

After we have estimated the model parameters, we can use the to calculate fitted values for $Y$

-   Fitted values of $Y$ are predictions of $Y$, given any values of $\overrightarrow{X}$

For linear regression, a fitted value for $Y$ is:

$$
\hat{Y} = \hat{\alpha} + \hat{\beta}_1 X_1 + \hat{\beta}_2 X_2 + ... + \hat{\beta}_k X_k
$$

<br />

### Types of Models

We will learn several different models, that deal with different types of $Y$ response variables:

-   Linear models for interval-level $Y$

-   Binary logistic models for dichotomous/binary $Y$

-   Multinomial and Ordinal logistic models for polytomous categorical $Y$

-   Negative binomial and Poisson Regression models for count $Y$

<br />

### Data Quality

For analysis to be useful, our data should be appropriate and of good quality

-   Measurement quality is good - lack of measurement error and inaccurate measures of concepts

-   Lack of missing data - which can cause loss of info, possible bias

These are important things to keep in mind, even if the course assumes that our data is of good quality

<br />

------------------------------------------------------------------------

[MY452A Homepage](https://kevinli03.github.io/notes/#MY452A_Regression_Analysis)

# Linear Model Basics

### Response Variables

Formally, linear regression requires that $Y$ is a continuous variable (interval or ratio level)

-   Something that behaves like a "proper number"

In reality - we do not always stick to this

-   Linear models are routinely used for counts and ordinal variables

-   However, dedicated models for these also exist, and it is better to use those than linear regression

Binary variables (True/false, 0/1, yes/no) also exist

-   Sometimes linear regression models are used

-   But, it is generally recommended to use logit models

Finally, there are $Y$ variables which have two un-ordered possible values

-   We should not use linear models for them

<br />

### Explanatory Variables

There are many different kinds of explanatory variables

-   Ratio, Categorical, Binary, etc.

We will cover these later and how they are used in linear regression

<br />

------------------------------------------------------------------------

[MY452A Homepage](https://kevinli03.github.io/notes/#MY452A_Regression_Analysis)

# Reading: Coursepack Ch. 1

Chapter 1, Kuha, J., and Lauderdale, B. (No date) *Applied Regression Analysis: Coursepack*

<br />

### Key Points

Regression is focused on the associations between variables

-   There is an association between variables, when knowing one variable helps you predict the other variable

<br />

If we know an association, what does that tell us about causality?

-   Not much. Correlation ≠ Causation

<br />

### Data and Variables

Information analysed by statistical methods is a set of data

-   Data consists of variables, or a set of units (observations)

-   Units may be countries, organisations, individuals, etc. - anything we are interested in analysing

-   Variables represent measurements of some characteristics of the units

<br />

Quantitative data is encoded in numbers - however, there are several different types of data based on how they are measured:

-   Nominal variables - where numbers simply represent labels for different categories. These categories cannot be put into any order

    -   Ex. countries - USA, China, EU (these cannot be ordered)

-   Ordinal variables - where numbers simply represent labels for different categories, but these categories can be ordered

    -   Ex. bad, neutral, good (these are ordered categories)

-   Interval scale variables - where the real numbers are meaninful, and the distance between units is equal

    -   Ex. 2-1 = 1, and 5-4 = 1. All units are 1 apart

<br />

There is another distinction we should pay attention to: continuous or discrete

-   Continuous variables can be indefinitely divided into smaller chunks

    -   Ex. between 4 and 5 is 4.5, between 4 and 4.5 is 4.25, ..., and so on

-   Discrete variables are when the unit of measurement cannot be subdivided - you can only have certain values:

    -   Ex. you have 1,2,3,4,5, but 1.5 makes no sense

    -   Nominal and Ordinal variables usually fit into this category

<br />

### Association and Regression

Regression is focused on the associations between variables

-   There is an association between variables, when knowing one variable helps you predict the other variable

-   Association can also be expressed as correlated variables

Sometimes, the two variables are treated on equal footing

-   However, often, it is more natural to think of one as predicting the other

-   The explanatory variable doing the predicting is labelled $X$, and $Y$ is the outcome response variable

<br />

We often look at more than 2 variables

-   We will often include multiple explanatory variable $X$

-   This allows us to examine the association between 2 variables, while controlling/holding constant the other variables

<br />

### Association vs. Causality

If we know an association, what does that tell us about causality?

-   Not much. Correlation ≠ Causation

-   To find causality, you need a counterfactual - what would've happened if the treatment was not assigned

Regression methods estimate associations, but they typically do not estimate causal effects

-   The only way to address causality in a regression model is if every, and I mean every, possible difference is controlled for

<br />

------------------------------------------------------------------------

[MY452A Homepage](https://kevinli03.github.io/notes/#MY452A_Regression_Analysis)

# Reading: Coursepack 3.1-3.3

Section 3.1-3.3, Kuha, J., and Lauderdale, B. (No date) *Applied Regression Analysis: Coursepack*

<br />

### Key Points

In social science, models take the form as follows:

$$
Y = \alpha + \beta X + \epsilon
$$

<br />

All models are wrong because the world and humans are very complex. However, the simplification could allow us to capture the essentials, and leave out the inessentials

<br />

### Statistical models

Models link two measurable quantities $X$ and $Y$ with an equation, based on a set of parameters:

$$
Y = \alpha + \beta X
$$

1.  Estimating the parameters $\alpha$ and $\beta$ can be done by looking at data regarding $X$ and $Y$
2.  Once estimates of the paremters are available, we are able to predict the value of $Y$ given any value of $X$

<br />

### Models in Social Sciences

In social science, models take the form as follows:

$$
Y = \alpha + \beta X + \epsilon
$$

Where $\epsilon$ is the error term. Why?

-   Humans are random - they do not behave exactly as mathematics says they will

-   So, this additional term explains the randomness in human actions

Remember - all models are wrong because the world and humans are very complex. However, the simplification could allow us to capture the essentials, and leave out the inessentials

<br />

------------------------------------------------------------------------

[MY452A Homepage](https://kevinli03.github.io/notes/#MY452A_Regression_Analysis)

# Reading: Gelman Ch. 1

Chapters 1-3, Gelman et al (2022) *Regression and Other Stories*

<br />

### Key Points

[**Regression**]{.underline} is a method that allows researchers to summarise how predictions or average values of an outcome, vary across individuals defined by a set of predictors

<br />

[**Bayesian Inference**]{.underline}

-   Bayesian Inference is an approach to stats, which incorporates prior information into inferences

-   Thus, it goals beyond the goal of summarising existing data

-   We first make an assumption, that makes us a prior distribution

-   Then we analyse our data in regards to the prior distribution

-   On the positives, bayesian gives us more reasonable results, that can be used to make predictions

<br />

### Statistics and Regression

There are 3 challenges of statistical inference:

1.  Generalising from sample to population
2.  Generalising from treatment to control group
3.  Generalising from observed measurements to the underlying features of interests

<br />

Regression is a method that allows researchers to summarise how predictions or average values of an outcome, vary across individuals defined by a set of predictors

Most important uses of regression are:

-   Prediction: modelling existing observations and forecasting new data

-   Exploring associations: summarising how a set of variables predicts the outcome

-   Extrapolation: adjusting for known differences between a sample and the population

-   Causal inference: comparing outcomes under treatment or control

<br />

### Building Models

Regressions can be used for causal inference

-   Regression can be used to estimate a relationship of interest between 2 (or more) variables

-   Regressions can also include control variables, to adjust for differences between treatment and control groups

-   However, we have to be careful about assigning causation - since correlation ≠ causation (we will explore this later in detail)

<br />

Building models and statistical analysis cycles through 4 steps

1.  Model building: start with the simple linear models, and expanding through additional predictors, interactions, transformations
2.  Model fitting: estimating regression coefficients, which often includes data manipulation, programming, and the use of algorithms
3.  Understanding model fits, which involves investigating the connections between measurements, parameters, and the underlying objects of the study
4.  Criticism: consider what is wrong with the model, and look to improve it

And cycle back, until there are minimal criticisms

<br />

An key challenge is the be critical - without being over critical

-   Models are simplifications, they will never be perfect

-   But, we need a model that is accurate and realistic enough to draw conclusions from them

<br />

### Classical and Bayesian Inference

Fitting models involves three concerns: what information is being used, what assumptions are being made, and how estimates are being interpreted

<br />

[Information]{.underline}

-   Information is the starting point for any regression problem

-   We can visualise relationships with charts such as scatterplots

-   We also typically know something about how the data was collected

-   Information should be available on what data was observed at all (the sampling process)

-   We also have prior knowledge from the literature on similar studies - but also note, research tends to over-estimate effect sizes due to pressure to find significant results

<br />

[Assumptions]{.underline}

-   First, we have to make an assumption on how $x$ and $y$ are related

    -   Often, we typically assume they are linear

    -   But often, this is not the case, and we may need transformations

-   Second, we have to consider the sampling process, and how this influences our abilities to make claims

-   Third, we have to be concerned with measurement accuracy - is our data collected properly and accurately? Is our measurement a good measurement of what we are interested in?

<br />

[Classical Inference]{.underline}

-   Traditional approach of stats is using our model, not using prior information, and getting estimates and predictions

-   This is called "frequentist" statistics

-   This includes unbiasedness, confidence intervals, conservatism, etc.

<br />

[Bayesian Inference]{.underline}

-   Bayesian Inference is an approach to stats, which incorporates prior information into inferences

-   Thus, it goals beyond the goal of summarising existing data

-   We first make an assumption, that makes us a prior distribution

-   Then we analyse our data in regards to the prior distribution

-   On the positives, bayesian gives us more reasonable results, that can be used to make predictions

-   On the negatives, it requires additional information - the prior distribution

<br />

### Computing Regression

Classical statistical regression is done in R in the following way:

```{r, eval = FALSE}
model <- lm(y ~ x, data = mydata)
```

<br />

Bayesian regression is done in R in the following way:

```{r, eval = FALSE}
model <- stan_glm(y ~ x, data = mydata)
```

Sometimes, Bayesian regression may perform slowly in R for large models. We can make it faster with optimising mode:

```{r, eval = FALSE}
model <- stan_glm(y ~ x, data = mydata, algorithm = "optimizing")
```

<br />

For switching between Classical and Bayesian regression, the methods are almost the same syntax, just replace "lm" with "stan_glm"

<br />

------------------------------------------------------------------------

[MY452A Homepage](https://kevinli03.github.io/notes/#MY452A_Regression_Analysis)
