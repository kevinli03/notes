---
title: "Bivariate and Multivariate OLS"
subtitle: "Session 2, GV481"
output:
  html_document:
    df_print: paged
    toc: yes
    toc_float:
      collapsed: true
    toc_depth: 4
    theme: lumen
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{=html}
<style>
  body .main-container {
    max-width: 1100px;
    font-size: 12pt;
  }
</style>
```
[GV481 Homepage](https://kevinli03.github.io/notes/#GV481_Quantitative_Analysis)

Week 1, Quantitative Analysis for Political Science

-   Title: Bivariate and Multivariate OLS

-   Topics:

-   Readings:

    1.  Chapters 4 and 5, Mesquita et al (2021) *Thinking Clearly with Data: A Guide to Quantitative Reasoning and Analysis*

    2.  Chapters 2.1-2.14, Cunningham, S. (2021) *Causal Inference: The Mixtape*

    3.  Fowler, A. (2018) 'Do Shark Attacks Influence Presidential Elections', *The Journal of Politics*, 80(4), pp. 1423-1437

    4.  Suryanarayan, P. and Steven, W. (2021) 'Slavery, Reconstruction, and Bureaucratic Capacity in the American South', *The American Political Science Review*, 155(2), pp. 568-584

    5.  Haber, S. and Victor, M. (2011) 'Do Natural Resources Fuel Authoritarianism? A Reappraisal of the Resource Curse', *The American Political Science Review*, 105(1), pp. 1-26.

<br />

------------------------------------------------------------------------

[GV481 Homepage](https://kevinli03.github.io/notes/#GV481_Quantitative_Analysis)

# Key Points

The [**best linear predictor**]{.underline} is the function $Y = \alpha + \beta X$ that best predicts $Y$.

-   Where $\alpha$ and $\beta$ are parameters we estimate from a dataset of values

We can estimate $\beta$ with the [**OLS estimator**]{.underline}, as follows:

$$
\hat{\beta} = \frac{\sum (x_i - \bar{y}) (x_i - \bar{x}) }{\sum (x_i - x)^2}
$$

<br />

In our model $\hat{Y} = \alpha + \beta X$, the [**interpretation**]{.underline} is as follows

-   coefficient $\alpha$ is the predicted $Y$ when $X=0$

-   Coefficient $\beta$ is the predicted change in $Y$ given a one unit increase of $X$

-   You will need to know how $Y$ and $X$ are measured to make this meaningful.

If one of the measurements is not very meaningful (ex. a 0-100 scale of democracy), we can also do another [**interpretation in terms of standard deviation**]{.underline}

-   The predicted change in $Y$ in terms of standard deviation, with one standard deviation increase in $X$

-   This value is equal to: $\Delta y = (SD_X \times \beta_1)/SD_Y$

-   Anything more than 0.31 standard deviation change in $Y$ is considered quite large

<br />

[**Properties of the OLS Model**]{.underline}:

-   All the residuals should add up to 0. Mathematically: $\sum[y_i-(\hat{\beta}_0 + \hat{\beta}_1 x_i)] = 0$

-   We can predict the sample mean of $y$ as follows: $\bar{y} = \hat{\beta}_0 + \hat{\beta}_1 \bar{x}$

<br />

[**Multivariate models**]{.underline} take the following form:

$$
Y = \alpha + \beta_1 X_1 + \beta_2 X_2 + ... + \beta_nX_n +\epsilon
$$

Now, $\beta_1$ represents the expected change of $Y$ given a one unit of $X_1$, given other variables $X_2, X_3, ...$ are held constant

The $\beta_1$ has a different estimation formula:

$$
\hat{\beta_1} = \frac{ \sum \hat{r}_{i1} y_i}{ \sum \hat{r}_{i1}^2}
$$

Where $\hat{r}_{i1}$ are the residuals obtained from regressing $X_1$ on $X_2$. This gives the part of $X_1$ that is uncorrelated with $X_2$

<br />

To deal with a non-linear relationship, we can use a [**polynomial transformation**]{.underline}.

$$
\hat{Y} = \alpha + \beta_1 X + \beta_2 X^2 ...
$$

-   If we want higher degree transformation, we can add another term

-   Higher polynomials may fit the data better, but may overfit, and be hard to interpret

<br />

[**Overfitting**]{.underline} essentially means that within our training data, the model performs amazingly, but with new data on $X$ to predict $Y$, it performs poorly

-   This is because it is fit too closely to our training data, which means it is only good at predicting the existing data, not new data

-   Remember, our training data is just a sample from a population. New data drawn from the population will not be exactly the same. We are interested in predicting $Y$ of the population, not of our specific training sample

In data science, we can avoid overfitting through either making training/testing sets, or cross-validation

<br />

------------------------------------------------------------------------

[GV481 Homepage](https://kevinli03.github.io/notes/#GV481_Quantitative_Analysis)

# OLS Regression

### Background

The best linear predictor is the function $Y = \alpha + \beta X$ that best predicts $Y$.

-   This takes the form of a linear line - a straight line

-   This should give us our best guess for $Y$, given a value of $X$

<br />

How do we choose this line, specifically the coefficient $\beta$ ?

-   We choose the line that minimises the sum of the squared prediction errors

-   A squared prediction error is the difference between the actual $y$ value at a point, and its estimate $\hat{y}$, then squared

    -   I.e. $(y - \hat{y})^2$

-   Then, we sum up all these squared errors: $\sum (y - \hat{y})^2$

-   We choose the line that minimises this value

<br />

The coefficient $\beta$ tells us the following:

-   The sign of beta tells us the direction of relationship (positive or negative)

-   $\beta$ itself represents the expected change in $Y$, given an increase of one unit of $X$

<br />

### OLS Estimator

Of course, drawing every possible linear line, and measuring the sum of squared errors, is very inefficient.

One way to estimate this with much less effort but strong accuracy is the ordinary least squares estimator. The procedure is as follows:

1.  Sample units from the population, and collect data on both $X$ and $Y$ for each individual in the sample
2.  Find $\alpha$ and $\beta$ that minimise the sum of squared errors

The formula for the $\beta$ in OLS is as follows:

$$
\hat{\beta} = \frac{\sum (x_i - \bar{y}) (x_i - \bar{x}) }{\sum (x_i - x)^2}
$$

<br />

### Application: Resource Curse

Question: are oil-rich countries less likely to be democratic, than non-oil countries?

-   Or in other words, is there a correlation between large oil revenues, and democracy

Our equation is as follows: Democracy $= \alpha + \beta_1$Fiscal Reliance $+ \epsilon$

-   The sign of $\beta_1$ tells us the direction of the relationship between the two variables

Remember: $X$ is our explanatory variable, $Y$ is the outcome variable

<br />

### Interpretation of OLS Model

Haber and Menaldo (2011) collected data for some countries on regime type and oil reliance.

To implement in R, you do the following:

```{r, eval = FALSE}
model <- lm(democracy ~ fiscalreliance, data = mydata)
```

The output is as follows:

![](figure2.1.png){width="100%"}

<br />

This tells us that for every one unit increase of fiscal reliance, democracy decreases by -0.9594.

-   But what does this actually mean? We need to know how fiscal reliance and democracy is measured

-   Fiscal reliance is measured between 0 and 100 percent. Democracy is measured between 0 and 100, where 100 means more democracy

That means, a 1 percentage point increase in fiscal reliance on oil, there is a -0.9594 predicted change in democracy.

<br />

It is still hard to understand what this means in terms of democracy - since it is such an arbitrary scale

-   So instead, we often use standard deviation change

-   Why? if we have normally distributed variables, we know that 2/3 of the distribution is within one standard deviation. 2 standard deviations should cover 95% of the distribution of the variable.

-   Thus, we can use standard deviation to see how drastic the change is, in respect to the variable

How do we get $Y$ in respect to 1 standard deviation increase of $X$?

-   $Y$ changes by $(SD_X \times \beta_1)/SD_Y$

So in this case, one standard deviation increase in fiscal reliance $X$, is associated with a 0.61 standard deviation increase in democracy

-   Generally anything above a 1/3 standard deviation change is considered a big change.

<br />

$\beta_0$ gives us the predicted value of $Y$, when $X=0$

-   So in this case, when fiscal reliance is 0, the expected value of democracy is 112.6634 (obviously, this doesn't make much sense)

-   As a product of OLS, sometimes, $\beta_0$ may not make much sense in terms of contextual interpretation

<br />

### Binary Y Variable

Generally, you should use a logistic model for binary $Y$. But, it is still possible in linear regression.

Interpretation of the coefficients is still mostly the same, with a slight changes:

-   $\alpha$ tells us the predicted probability of $Y=1$, when $X = 0$

<!-- -->

-   $\beta$ is now the predicted change in the probability of event $Y=1$, given one unit increase of $X$

-   Once again, we can do the expected standard deviation change in $Y$, given one standard deviation increase in $X$

<br />

------------------------------------------------------------------------

[GV481 Homepage](https://kevinli03.github.io/notes/#GV481_Quantitative_Analysis)

# Multivariate Models

### Introduction

The best linear predictor allows us to understand the expected change of $Y$, for every unit change in $X$

-   However, this approach is limited - since we are not accounting for other factors

-   Maybe $X$ is associated with something else which explains $Y$

-   To be sure of the $X$ impact on $Y$, we need to control other explanations of $Y$

For example, we might want to know what the expected change in democracy is, when fiscal reliance on oil increases, for countries with comparable levels of GDP

<br />

### Multivariate Regression

Now, our regression takes the form of:

$$
Y = \alpha + \beta_1 X_1 + \beta_2 X_2 + ... + \beta_nX_n +\epsilon
$$

Now, $\beta_1$ represents the expected change of $Y$ given a one unit of $X_1$, given other variables $X_2, X_3, ...$ are held constant

<br />

The $\beta_1$ has a different estimation formula:

$$
\hat{\beta_1} = \frac{ \sum \hat{r}_{i1} y_i}{ \sum \hat{r}_{i1}^2}
$$

Where $\hat{r}_{i1}$ are the residuals obtained from regressing $X_1$ on $X_2$. This gives the part of $X_1$ that is uncorrelated with $X_2$

<br />

### Application: Resource Curse

Now, let us look at the relationship between democracy and oil revenues, while controlling for GDP per capita:

-   Democracy = $\alpha + \beta_1$Oil Income $+ \beta_2$GDP per capita

In R, this is how you execute it:

```{r, eval = FALSE}
model <- lm(democracy_binary ~ totalincomepc_000 + gdppc, data = mydata)
```

The output is as follows:

![](figure2.2.png){width="100%"}

<br />

You would interpret as following:

-   Intercept $\alpha$ is the predicted probability of a country being a democracy, given total oil income is 0 and gdp per capita is 0

-   $\beta_1$ shows that for a \$1000 increase in total oil income per capita, and GDP per capita is held constant, there is an expected 6.9 percentage point decrease in the probability that a country is a democracy

    -   Std. Deviation change $(SD_X \times \beta_1)/SD_Y$ = $(2.808 \times -0.069)/0.47$

-   $\beta_2$ shows that for every \$1000 increase in GDP per capita, and total oil income per capita is held constant, then there is an expected 1.4 percentage point increase in the probability that a country is a democracy

    -   St. deviation change $(SD_X \times \beta_1)/SD_Y$

<br />

### Linearity in Linear Regression

Linear regression does not always mean a straight line

-   Linearity means linearity in parameters

-   Notice how each parameter is added together $Y = \alpha + \beta_1 X_1 + \beta_2 X_2 + ... + \beta_nX_n$

-   A non-linear relationship would be if $\beta$'s were multiplied or divided by each other

-   This does not mean that explanatory variables $X_1,...X_n$ cannot be multiplied or raise to powers

<br />

------------------------------------------------------------------------

[GV481 Homepage](https://kevinli03.github.io/notes/#GV481_Quantitative_Analysis)

# Reading: Mesquita et al

Chapters 4 and 5, Mesquita et al (2021) *Thinking Clearly with Data: A Guide to Quantitative Reasoning and Analysis*

<br />

### Key Points

If you want to figure out whether two variables are [**correlated**]{.underline}, you have to **observe variation** in both of them

-   Just observing variation in one variable is not enough

If there is no variation, that is just a fact, not a correlation

<br />

A [**regression equation**]{.underline} takes the following form:

$$
\hat{Y} = \alpha + \beta X
$$

The regression expresses a linear relationship between a dependent (outcome) $Y$ variable, and a independent (explanatory) $X$ variable

-   The dependent variable $Y$ is the outcome we are trying to describe, predict, or explain

-   The independent variable $X$ is what we are using to explain, describe, or predict $Y$

<br />

$\alpha$ and $\beta$ are [**regression parameters**]{.underline}

-   These parameters define the particular line we are drawing

-   $\alpha$ is called the intercept, the predicted value of $Y$ given $X=0$

-   $\beta$ is the slope - the predicted amount that $Y$ changes for each unit increase of $X$

The line that we get from estimating $\alpha$ and $\beta$ which minimise the sum of squared errors, is the OLS regression line of best fit.

<br />

To deal with a non-linear relationship, we can use a [**polynomial transformation**]{.underline}.

$$
\hat{Y} = \alpha + \beta_1 X + \beta_2 X^2 ...
$$

-   If we want higher degree transformation, we can add another term

-   Higher polynomials may fit the data better, but may overfit, and be hard to interpret

<br />

[**Overfitting**]{.underline} essentially means that within our training data, the model performs amazingly, but with new data on $X$ to predict $Y$, it performs poorly

-   This is because it is fit too closely to our training data, which means it is only good at predicting the existing data, not new data

In data science, we can avoid overfitting through either making training/testing sets, or cross-validation

<br />

### Correlation Requires Variation

If you want to figure out whether two variables are correlated, you have to observe variation in both of them

-   Just observing variation in one variable is not enough

If there is no variation, that is just a fact, not a correlation

-   For example, $x$% of students who drop out find school boring is a fact, as there is no variation in the variable "boring"

-   A correlation would be - the more boring a student finds school, the more likely they are to drop out

-   Note the key word - more - which shows variation and comparison

<br />

Dependent variable is the variable you are trying to forecast or explain

-   Dependent because it depends on something else

-   It is the outcome variable we are trying to explain

When selecting the dependent variable, make sure to pay attention to the variation

-   Don't only pick cases where the dependent variable is true, pick cases where there are both true and false values of the dependent variable, so there is variation

<br />

For example, let us identify the following statements as correlation or fact:

-   Most top performing schools have small student bodies (fact)

-   Married people are typically happier than unmarried people (correlation)

-   Among professionals, taller basketball players tend to have lower free-throw percentages than shorter players (correlation)

-   The locations in the US with the highest cancer rates are typically small towns (fact)

-   Order households are more likely to have lead paint than newer ones (correlation)

-   Most colds caught in Cook County are caught on cold days (fact)

<br />

### Regression

Correlation shows us the direction of the relationship between two variables

-   But, sometimes, we want to be more precise

-   How much does change in one variable reflect on the other variable? The "magnitude" of the relationship

The slope of a regression best fit line tells us both the direction of the correlation, and the magnitude of the correlation

<br />

A regression equation takes the following form:

$$
\hat{Y} = \alpha + \beta X
$$

The regression expresses a linear relationship between a dependent (outcome) $Y$ variable, and a independent (explanatory) $X$ variable

-   The dependent variable $Y$ is the outcome we are trying to describe, predict, or explain

-   The independent variable $X$ is what we are using to explain, describe, or predict $Y$

<br />

$\alpha$ and $\beta$ are regression parameters

-   These parameters define the particular line we are drawing

-   $\alpha$ is called the intercept, the predicted value of $Y$ given $X=0$

-   $\beta$ is the slope - the predicted amount that $Y$ changes for each unit increase of $X$

<br />

Of course, we don't want to just draw some random line with random $\alpha$ and $\beta$, that won't be very good at matching the data

-   We want to find the line that best fits the data

-   Generally, this means we want to minimise the sum of squared errors

-   Then, we find $\alpha$ and $\beta$ that does minimise the sum of squared errors

The line that we get from estimating $\alpha$ and $\beta$ which minimise the sum of squared errors, is the OLS regression line of best fit.

<br />

Why minimise the sum of squared errors, not the sum of absolute value errors, or sum of fourth power errors?

-   Minimising squared errors provides the best linear approximation to the conditional mean function

    -   Conditional mean function tells us the mean of some variable, conditional of the value of some other variable

    -   Basically, expected value of $Y$ given some $X$ value - $E[Y|X]$

-   Also, it is historical - since squares are easy to work with linear algebra. This was especially important when people used to do this manually

<br />

### Non-Linear Data

What if we want to use linear regression, but our data isn't really linear?

-   We can just use a linear regression, but the error will be larger

-   Or, we can use regression lines, but different lines for different parts of the data (basically split $x$ into chunks, and create regression lines for each chunk)

-   We could also transform our $X$ variables by squaring, cubing, or logging them

-   We could also add more variables to control for, which may improve our predictions

<br />

A square transformation may take the following form

$$
\hat{Y} = \alpha + \beta_1 X + \beta_2 X^2 ...
$$

-   If we want higher degree transformation, we can add another term

-   However, we don't necessarily want to add too many of these, even if it does marginally improve our predictive power

    -   We want our model to be simple to analyse, interpret, and communicate

    -   We want to avoid overfitting the data when we are concerned with prediction

-   We want to find the right balance of simplicity and accuracy

<br />

What is overfitting that I just mentioned previously?

-   Basically it means that within our training data, the model performs amazingly, but with new data on $X$ to predict $Y$, it performs poorly

-   This is because it is fit too closely to our training data, which means it is only good at predicting the existing data, not new data

    -   Remember, our training data is just a sample from a population. New data drawn from the population will not be exactly the same. We are interested in predicting $Y$ of the population, not of our specific training sample

In data science, we can avoid overfitting through either making training/testing sets, or cross-validation

<br />

------------------------------------------------------------------------

[GV481 Homepage](https://kevinli03.github.io/notes/#GV481_Quantitative_Analysis)

# Reading: Cunningham

Chapters 2.1-2.14, Cunningham, S. (2021) *Causal Inference: The Mixtape*

<br />

### Key Points

[**Bayes Rule**]{.underline}:

-   $P(A|B) = \frac{P(B|A) \times P(A)}{P(B)}$

-   Or decomposed: $P(A|B) = \frac{P(B|A) \times P(A)}{P(B|A)P(A) + P(B|A^c)P(A^c)}$

    -   That denominator is just derived from the law of total probability

<br />

[**Expected Value and Variance**]{.underline}

-   $E[X] = x_i f(x_i)$, where $x_i$ is a potential outcome, and $f(x_i)$ is the probability of that potential outcome

-   Variance $\sigma^2 = E[ (X-E[X])^2] = \frac{1}{n}\sum(x_i - E[X])^2$

<br />

[**Covariance**]{.underline} measures the amount of linear dependence between two random variables

-   Positive covariance means $X$ and $Y$ move in same direction, negative means opposite direction, 0 means no linear relationship

-   Covariance $\sigma_{XY}=\frac{1}{n}  \sum(x_i - \bar{x})(y_i - \bar{y})$

-   We know that $\sum(x_i - \bar{x})(y_i - \bar{y}) = \sum x_i y_i - n(\overline{xy})$ from above

-   Thus, covariance is also also $\sigma_{XY} =E[XY] - E[X] - E[Y]$

Covariance cannot be measured for magnitude. Let us use [**correlation regression**]{.underline}:

-   Correlation $r = \rho = \frac{Cov(X,Y)}{\sqrt{Var(X)Var(Y)}}$

-   We can rewrite as $\rho = \frac{\sigma_{XY}}{\sigma_X \sigma_Y}$

-   Correlation coefficient always between -1 and 1

<br />

Properties of the [**OLS Model**]{.underline} $Y = \hat{\beta}_0 + \hat{\beta}_1 X$

-   All the residuals should add up to 0. Mathematically: $\sum[y_i-(\hat{\beta}_0 + \hat{\beta}_1 x_i)] = 0$

-   We can predict the sample mean of $y$ as follows: $\bar{y} = \hat{\beta}_0 + \hat{\beta}_1 \bar{x}$

<br />

### Probability Theory

For detailed notes on probabiltiy, see my notes in GV4L8 or my notes in politicalscience.github.io

There are two types of random variables - discrete and continuous

-   Discrete - countable number of distinct outcomes: this can include both several categories of outcomes, as well as integer-only outcomes

    -   Ex. 12-sided die, it has 12 distinct outcomes, 1-12

    -   Ex. a Coin, it has 2 distinct outcomes, heads or tails

    -   Deck of cards - it has 52 distinct outcomes - each card

-   Continuous - infinite number of outcomes - basically, not only integers, but also decimals between each integer

    -   Ex. Gas Prices - it has an infinite number of outcomes - 0, 0.01, 0.012, 0.013... ∞

<br />

Statistical independence is when the probability of one event, is not affected by another occurring

-   Sampling with replacement makes each draw independent

-   Sampling without replacement makes each subsequent draw more likely, thus not independent

If we have two independent events $A$ and $B$, the probability of getting both events is $P(A) \times P(B)$

<br />

For detailed set theory, see my notes in GV4L8 or my notes in politicalscience.github.io

-   Basically, Complement of $A$, notated as one of$~A,A',A^c$, is everything not in $A$, but within the universal set $U$

    -   Thus, $A + A' = U$

-   Intersection of sets $A \cap B$ mean A and B - basically, only the elements that are both within $A$ and $B$

-    Union of sets $A \cup B$ means A or B - basically the combination of all elements that appear in either $A$, $B$, or in both sets

    -   $A \cup B = A + B + A \cap B$

<br />

Conditional probability is the probability that one event occurs, given another has already occurred:

-   For example, probability of $A$, given $B$ has occurred, is $P(A|B)$

-   $P(A|B) = P(A)$ if the events $A$ and $B$ are independent

-   $P(A|B) = P(A \cap B) / P(B)$

<br />

Bayes Rule: (see notes GV4L8 for derivation of Bayes' rule:

-   $P(A|B) = \frac{P(B|A) \times P(A)}{P(B)}$

-   Or decomposed: $P(A|B) = \frac{P(B|A) \times P(A)}{P(B|A)P(A) + P(B|A^c)P(A^c)}$

    -   That denominator is just derived from the law of total probability

<br />

For expanded notes on summation and product notation, see politicalscience.github.io

-   Mean $\mu_X = \frac{1}{n} \sum x_i$

-   Useful result: $\sum(x_i - \bar{x})^2 = \sum x_i^2 - n(\bar{x})^2$

    -   Pretty obvious - just decompose sum into 2 parts

-   We can apply that to two variables: $\sum(x_i - \bar{x})(y_i - \bar{y}) = \sum x_i y_i - n(\overline{xy})$

<br />

### Expected Value and Variance

For expanded notes on expected value and variance, see politicalscience.github.io or GV4L8

-   $E[X] = x_i f(x_i)$, where $x_i$ is a potential outcome, and $f(x_i)$ is the probability of that potential outcome

-   Variance $\sigma^2 = E[ (X-E[X])^2] = \frac{1}{n}\sum(x_i - E[X])^2$

<br />

### Covariance and Correlation

Covariance measures the amount of linear dependence between two random variables

-   Positive covariance means $X$ and $Y$ move in same direction, negative means opposite direction, 0 means no linear relationship

-   Covariance $\sigma_{XY}=\frac{1}{n}  \sum(x_i - \bar{x})(y_i - \bar{y})$

-   We know that $\sum(x_i - \bar{x})(y_i - \bar{y}) = \sum x_i y_i - n(\overline{xy})$ from above

-   Thus, covariance is also also $\sigma_{XY} =E[XY] - E[X] - E[Y]$

Covariance cannot be measured for magnitude. Let us use correlation coefficient:

-   Correlation $r = \rho = \frac{Cov(X,Y)}{\sqrt{Var(X)Var(Y)}}$

-   We can rewrite as $\rho = \frac{\sigma_{XY}}{\sigma_X \sigma_Y}$

-   Correlation coefficient always between -1 and 1

<br />

Model of the relationship between two variables $y = \beta_0 + \beta_1x + \epsilon$

-   Let us not worry too much about the error term $\epsilon$

Intuitively, the line gives the expected value of $Y$, given any value of $X$: $E[Y|X]$

-   This is because obviously, $Y$ will have a variation of potential outcomes - our $X$ is not a perfect linear predictor of $Y$

-   So our equation outputs the expected value of $Y$ given any value of $X$

$\beta_1$ is selected by reducing the sum of squared errors. After some complex math not essential to this course, we get an estimate for the paremter:

$$
\hat{\beta}_1 = \frac{\sum (x_i - E[X]) (y_i - E[Y])}{\sum(x_i - E[X])^2}
$$

$$
\hat{\beta}_1 = \frac{ \text{sample } Cov(x_i, y_i)}{\text{Sample} Var(x_i)}
$$

We usually let the computer do all this.

<br />

Properties of the OLS Model:

-   All the residuals should add up to 0. Mathematically: $\sum[y_i-(\hat{\beta}_0 + \hat{\beta}_1 x_i)] = 0$

-   We can predict the sample mean of $y$ as follows: $\bar{y} = \hat{\beta}_0 + \hat{\beta}_1 \bar{x}$

<br />

------------------------------------------------------------------------

[GV481 Homepage](https://kevinli03.github.io/notes/#GV481_Quantitative_Analysis)

# Reading: Fowler & Hall

Fowler, A. and Hall, A. B. (2018) 'Do Shark Attacks Influence Presidential Elections', *The Journal of Politics*, 80(4), pp. 1423-1437

<br />

### Key Points

There is a prominent claim made in 1916, that shark attacks influence presidential elections

-   This paper studies this claim made by Achen (2002) and Bartels (2016)

Conclusions

1.  Finds no evidence of this relationship
2.  Achen and Bartels's couty-level findings become much weaker under different specifications
3.  Town-level findings of those 2 significantly shrinks when we correct errors, and does not hold for other beach counties
4.  Implementing placebo counterfactual tests show that their result was likely to arise, even if shark attacks do not influence elections

<br />

### Summary

Aachen and Bartels (2002, 2016) found a relationship between shark attacks and voter behaviour in the 1916 US presidential election in new jersey

-   This has been repeated by many scholars, and entered public media

-   This claim has key implications on the rationality of voters

    -   The literature shows that voters appear politically ignorant

    -   However, in aggregate, voters do tend to respond reasonably to government performance and candidate ideology and quality

-   Achen and Bartels themselves say that their finding shows voters are irrational, and thus, our conventional view of democratic accountability may be severely undermined

-   However, all this debate relies on one key assumption - that the claim Achen and Bartels made is correct

-   There is evidence that this link between shark attacks and voter decisions is not strong enough to generate these types of discussion

<br />

Paper proceeds in four parts

1.  Assemble data on fatal shark attacks and county-level vote returns between 1872 and 2012 (for presidential elections). They find very small relationship with no significance
    -   Implement differences in differences approach with state-election and county-period fixed effects. Fixed effects accounts for maybe, the fact, that beach counties are related and have similar support for a certain political group. The periods account for fluctuations between political parties that naturally occur
    -   Result finds no statistical significance from Attacks on voter decision, and effect is tiny
    -   Even when we change attacks from yes/no to number of attacks, there is still no statistically significant result (and effect is tiny)
    -   Even when we only consider shark attacks in election years (myopic voters), there is still no statistically significant result (and effect is tiny)
    -   Even when we only consider shark attacks in election years where the incumbent seeks re-election, we get no statistically significant result (and effect is tiny)
2.  Assess the robustness of Achen and Bartel's countylevel results from New Jersey - and shows how their result reflects more about the 1912 election which they used as a control, rather than the 1916 election with shark attacks (1912 is an outlier)
    -   Achen and Bartel exlclude essex county, but this does not affect results much
    -   Achen and Bartels hypothesise effects of shark attacks only appear in beach counties on the jersey shore. This makes sense. But we could also only focus on the two counties with shark attacks. When doing the second approach, there is no statistical significance
    -   If we include all counties that border the coast (not just jersey coast), we also have a non-statistically significant effect
    -   Achen and Bartels control for a few counties, which they claim have political machines which would alter the results. However, if we use an alternative index of political machines, we get a statistically insignificant results
    -   Basically, unless the exact decision choices and data was used as Achen and Bartels did, the result is not replicated, suggesting that this result is not robust
3.  Achen and Bartels's town level result from Ocean County does not hold for other beach counties of New Jersey
    -   Achen and Bartels explored Ocean County specifically, dividing the towns into beach and not beach, and seeing if the effect was different. They found that beach towns experienced more effect of attacks -\> voting.
    -   However, they make a few mistake with the boundaries at the time, If we correct the mistake and change all the town boundaries to the original boundaries, the results are no longer statistically significant
    -   If there claims are correct in Ocean County, we should expect this pattern in other beach counties. However, we do not find significant evidence of this.
4.  Demonstrate that the standard errors of Achen and Bartel's empriical strategy considerably overstates the level of statistical confidence
    -   The paper only finds an estimate as large as the New Jersey estimate in 32% of cases
    -   An only 27% cases are statistically significant
    -   Thus, we cannot be confident this relationship is not just statistical random variation.

<br />

------------------------------------------------------------------------

[GV481 Homepage](https://kevinli03.github.io/notes/#GV481_Quantitative_Analysis)

# Reading: Suryanarayan & Steven

Suryanarayan, P. and Steven, W. (2021) 'Slavery, Reconstruction, and Bureaucratic Capacity in the American South', *The American Political Science Review*, 155(2), pp. 568-584

<br />

### Key Points

<br />

------------------------------------------------------------------------

[GV481 Homepage](https://kevinli03.github.io/notes/#GV481_Quantitative_Analysis)

# Reading: Haber & Victor

Haber, S. and Victor, M. (2011) 'Do Natural Resources Fuel Authoritarianism? A Reappraisal of the Resource Curse', *The American Political Science Review*, 105(1), pp. 1-26.

<br />

### Key Points

<br />

------------------------------------------------------------------------

[GV481 Homepage](https://kevinli03.github.io/notes/#GV481_Quantitative_Analysis)
