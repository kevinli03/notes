---
title: "Quantitative Thinking and Correlations"
subtitle: "Session 1, GV481"
output:
  html_document:
    df_print: paged
    toc: yes
    toc_float:
      collapsed: true
    toc_depth: 4
    theme: lumen
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{=html}
<style>
  body .main-container {
    max-width: 1100px;
    font-size: 12pt;
  }
</style>
```
[GV481 Homepage](https://kevinli03.github.io/notes/#GV481_Quantitative_Analysis)

**Week 1, GV481 Quantitative Analysis for Political Science**

-   Title: Quantitative Thinking and Correlations

-   Topics: Statistical Inference, Covariation, Correlation

-   Readings:

    1.  Chapter 1, Mesquita et al (2021) *Thinking Clearly with Data: A Guide to Quantitative Reasoning and Analysis*

    2.  Chapter 2, Mesquita et al (2021) *Thinking Clearly with Data: A Guide to Quantitative Reasoning and Analysis*

<br />

------------------------------------------------------------------------

[GV481 Homepage](https://kevinli03.github.io/notes/#GV481_Quantitative_Analysis)

# Key Points

[**Correlation**]{.underline} is the extent to which two features tend to occur together

-   Positive correlation means as $X$ rises, $Y$ rises

-   Negative correlation means as $X$ rises, $Y$ falls

-   No correlation means that as $X$ rises, $Y$ doesn't have a linear pattern

<br />

[**Mean**]{.underline}/Average $\mu, \bar{x}, E[X]$ of a variable is:

$$
\mu = \bar{x} = E[X] = \frac{\sum x_i}{n}
$$

[**Variance**]{.underline} $\sigma^2$ is as follows (square root for [**standard deviation**]{.underline} $\sigma$):

$$
\sigma^2_X = \frac{\sum (x_i - \bar{x})^2}{n}
$$

<br />

[**Covariance**]{.underline} $\sigma_{X,Y}$ is as follows - we cannot interpret the actual value, only the sign:

$$
cov(X,Y) = \frac{\sum (x_i - \bar{x})(y_i - \bar{y})}{n} = E[ \space (x_i - E[X]) \times (y_i - E[Y]) \space ]
$$

[**Correlation coefficients**]{.underline} $r, \rho_{X,Y}$ are a normalised form of covariance (always between -1 and 1), so we can interpret the strength of correlation:

$$
corr(X,Y)=r = \rho_{X,Y} = \frac{cov(X,Y)}{\sigma_X \sigma_Y}
$$

$r^2$ is the square of the correlation coefficient - tells us what percentage of the variation of $Y$ is explained by $X$

$$
r^2 = \left( \frac{cov(X,Y)}{\sigma_X \sigma_Y} \right )^2$$

<br />

[**Linear best fit**]{.underline} lines take the following form:

$$
y = \alpha + \beta x
$$

The slope $\beta$ of the best fit line is related to covariance. Tells us the magnitude of the effect of $X$ on $Y$

$$
\beta_1 = \frac{cov(X,Y)}{\sigma^2_X}
$$

<br />

------------------------------------------------------------------------

# Statistical Inference

### Introduction to Inference

This course is about inference: using facts you know (data) to learn facts you don't know (what we are interested in)

-   Data generating process: process generating the data we observe.

    -   Ex. Height in the module - data generating process depends on who got into the module, who went to the lecture, who applied to the LSE, etc.

-   If you know the data generating process, you can use probability to make an inference on the observed data

-   Opposite is also true - you can use probability with the observed data to determine the data generating process

<br />

Probability vs. Inference:

-   Probability: If I have a coin, what is the probability I get a head?

-   Inference: Given I have $x$ number of heads, and $z$ number of tails, is this coin a fair coin?

<br />

### 3 Types of Inference

There are 3 types of inference: descriptive, predictive, and causal inference

-   Descriptive inference: we want to learn about the prevalence of a feature, or a correlation between 2 features in the population

    -   Basically - what is going on in the world

-   Predictive inference: we want to learn about the value of a feature, that we do not observe

    -   Ex. predicting who will vote, when it isn't election time yet (in the future)

-   Causal Inference: we want to learn about the causal effect of a treatment, on an outcome, on the population

    -   How does $x$ cause $y$?

    -   Ex. does disclosing information about corruption, cause an improvement in electoral accountability

<br />

------------------------------------------------------------------------

[GV481 Homepage](https://kevinli03.github.io/notes/#GV481_Quantitative_Analysis)

# Covariance and Correlation

### Introduction

We want to learn about the relationship between two features of the world

-   Ex. Are oil producer countries more likely to be autocratic?

The correlation between two features - is the extent, to which they tend to occur together

-   When we are more likely to observe feature $Y$ when feature $X$ is present, the correlation between $Y$ and $X$ is positive

-   When we are less likely to observe feature $Y$ when feature $X$ is present, the correlation between $Y$ and $X$ is negative

-   When there is no relationship between observing $Y$ and $X$, there is no correlation

We can visually see relationships between variables by graphing them in a scatterplot.

<br />

### Measure of Covariance

Covariance is - to what extent, does $X$ and $Y$ vary together:

$$
Cov(X,Y) = E[(X_i - E[X])(Y_i - E[Y])
$$

Intuitively:

-   $X_i$ is some value of $X$, while $E[X]$ is the mean of $X$

-   Thus, $X_i - E[X]$ is the distance between any point $X_i$ and its mean $E[X]$

-   $Y_i - E[Y]$ is the same, but for another feature $Y$

-   Multiply the two together for every point

-   Then, we find the expectation of the products (divide by number of observations $n$)

<br />

Let us look at the signs of the covariance:

-   If $X_i - E[X]$ tends to be positive while $Y_i - E[Y]$ is also positive, the product will be positive, hence a positive correlation (when one increases, the other also increases)

-   If one is positive and one is negative, the product will be negative, hence a negative correlation (when one increases, the other decreases)

<br />

### Measure of Correlation

Covariance does not show how strong a correlation is.

-   Why? If we change the way we measure $X$ and $Y$ (such as multiplying each variable by 100), the product would be much higher

-   But, the relationship is still proportional to each other

Thus, we need some other, normalised measure, to show correlation

-   We can normalise by the variance of each variables's standard deviation $\sigma = \sqrt{var(x)}$

<br />

Thus, the correlation coefficient is as follows:

$$
Corr(X, Y) = \frac{Cov(X,Y)}{\sqrt{Var(X)Var(Y)}} = \frac{Cov(X,Y)}{\sigma_X \sigma_Y}
$$

<br />

Correlation coefficient is always between -1 and 1

-   A larger negative number means a stronger negative correlation, with -1 being a perfect negative correlation

-   A larger positive number means a stronger positive correlation, with 1 being a perfect positive correlation

-   0 means no correlation

<br />

------------------------------------------------------------------------

[GV481 Homepage](https://kevinli03.github.io/notes/#GV481_Quantitative_Analysis)

# Linear Predictors

### Introduction

The best linear predictor, is the linear function of $X$, that best predicts $Y$

-   A linear function takes the form $Y = \alpha + \beta X$

-   Linear functions are straight lines

-   $\alpha$ is the $Y$ intercept - the value of $Y$ given $X = 0$

-   $\beta$ is the slope - for every increase of 1 in $X$, the $Y$ value increases by this much

Once we know $\alpha$ and $\beta$, this will give us the best guess of $Y$, given a value of $X$

<br />

### Estimation of Coefficients

How do we get the values of coefficients $\alpha$ and $\beta$?

-   We want to find the line that minimises the squared prediction errors - this produces the best linear estimate

What are the squared prediction errors?

-   If we draw a predictive line, we will have a predicted $\hat{y}$, and a actual $y$ value in the data

-   The difference between $y$ and $\hat{y}$ is the error of the prediction

-   Why square the error? Well we don't care if the error is in a positive or negative direction - so we square them to get rid of the signs

-   We sum every squared error to get the sum of squared errors

-   Then, we find the line with the lowest squared errors - that is the best linear prediction

<br />

### Interpretation of Coefficients

We are primarily interested in $\beta$, the slope

-   This is because it tells us both the direction of the correlation, and the magnitude of the correlation

-   If we are for sure (generally 95% sure) that $\beta>0$, then the correlation is positive .

-   If we are for sure (generally 95% sure) that $\beta < 0$, then the correlation is negative

-   If we cannot be sure of either, then we cannot conclude there is a correlation

<br />

### Formula for Coefficient

$\beta$ is related to both covariance that we previously discussed:

$$
\beta = \frac{cov(X,Y)}{\sigma^2_X} = \frac{\sum\limits_{i=1}^N (x_i - \bar{x})(y_i - \bar{y})}{\sum\limits_{i=1}^N (x_i - \bar{x})^2}
$$

<br />

------------------------------------------------------------------------

[GV481 Homepage](https://kevinli03.github.io/notes/#GV481_Quantitative_Analysis)

# Reading: Mesquita Ch. 1

Chapter 1, Mesquita et al (2021) *Thinking Clearly with Data: A Guide to Quantitative Reasoning and Analysis*

<br />

### Key Points

-   We must be careful before making causal claims - make sure that alternative explanations are controlled for, and that we do not mistake correlation as causation

-   Data alone cannot tell us the answer - we must use our domain knowledge

-   Always be critical - our initial hunches are often wrong, and just because there is data, does not mean an argument is foolproof

<br />

### Cautionary Tales

Scenario: Ethan's child, Abe, tested positive for a disease in test $A$, but tested negative for disease in test $B$ (2 tests for same diagnosis)

-   Doctors said the positive test $A$ is 80% accurate, doctor says it is a strong diagnosis

-   Ethan wants to be sure, so he does his own research

-   There are 2 types of measures of effectiveness for diagnostic tests:

    -   False negative - how frequently the test says a person is healthy, when they are actually sick

    -   False positive - how frequently the test says a person is sick, when they are actually healthy

-   Ethan's doctor said 80% accurate - but we aren't sure if that is a 20% false negative or 20% false positive

-   Ethan does more research

    -   He finds that the test for the disease has false negative rate of 20%, but has a false positive rate of 50%

    -   Basically, people who don't have the disease, are just as likely to test positive as testing negative

    -   His research for the test of test $B$ shows a much lower false negative and false positive rate

-   Before getting test results, let us say his chance of having disease $A$ is 1 in 100. However, with the new information, the actual likelihood is about 1 in 1,000

-   After getting a second opinion, the new doctor agreed, and it was true - Abe did not have the disease. This shows how the data - an 80% success rate - can be very misleading

<br />

Scenario: You want to protest the government (for whatever reason). Should you adopt a non-violent approach, or encourage violence? Let us put morals aside - ask which is more effective at changing government behaviour, and what are the potential risks (jail, injury, death)?

-   Let us look at some evidence:

    -   Comparing anti-government movements across the world over time, those that use less violence are more likely to achieve their goals

    -   Personal risks of violent protest are greater - making concerns about jail, injury, and death much higher

-   This seems convincing - however, let us think a little more carefully about this

    -   People are more likely to engage in non-violent protests, when the government is more willing to negotiate

    -   Or maybe, people are more likely to use non-violent protests when their cause is more popular with the population

-   The point is - while there may be correlation (non-violence is associated with more change in government behaviour), we cannot be sure if there is causality

    -   We would need to hold all other factors equal to isolate. the effect

<br />

Scenario: Police on foot-patrol reduced the fear of violent crime in Newark, NJ. Why was this the case?

-   Kelling and Wilson created a theory: crime sprials downward - a small crime, like a broken window leads to more emboldened children, which leads to families moving out and more criminals moving in, and so on...

    -   The claim is - if an area is "disorderly" (such as visible graffitti), people are more likely to do "undesirable behaviour"

-   Thus, police departments started on minimising disorder to reduce violent crime - focusing heavily on the "little crime"

    -   NYC started this programme in the 90s, and saw a drastic reduction in crime

    -   Later analysis showed that neighbourhoods where police were more focused on the "small things" saw a larger reduction in crime

-   So this means that the theory is correct right? There is a causal relationship? â€“ not so quickly

    -   Which precincts were likely to have had more focus of the police? Of course, the most dangerous ones - especially right before the implementation of the policies

    -   However, crime is very variable - and often "reverts back to the mean" and corrects

    -   So, we can't be sure if the dropping of crime in these neighbourhoods was a result of the policing, or just a natural return to the mean

-   Further research showed that looking at areas with similar levels of crime, the policing approach actually had the opposite effect - it had an increase in violent crime where the broken windows approach was implemented

<br />

### Thinking and Data

As the tales above show, data alone cannot tell us the answer

-   Just because data is involved, does not mean the argument is correct - in fact, quantitative skills without clear thinking is dangerous

-   We must combine data analysis with clear thinking

-   Furthermore, our initial hunches are often wrong - so we should always be sceptical

<br />

The goal of this book is to internalise the ways of thinking

-   Be critical - be sceptical, don't just accept data for what it is - try to poke holes in arguments

<br />

------------------------------------------------------------------------

[GV481 Homepage](https://kevinli03.github.io/notes/#GV481_Quantitative_Analysis)

# Reading: Mesquita Ch. 2

Chapter 2, Mesquita et al (2021) *Thinking Clearly with Data: A Guide to Quantitative Reasoning and Analysis*

<br />

### Key Points

Correlation is the extent to which two features tend to occur together

-   Positive correlation means as $X$ rises, $Y$ rises

-   Negative correlation means as $X$ rises, $Y$ falls

-   No correlation means that as $X$ rises, $Y$ doesn't have a linear pattern

<br />

Mean/Average $\mu, \bar{x}, E[X]$ of a variable is:

$$
\mu = \bar{x} = E[X] = \frac{\sum x_i}{n}
$$

Variance $\sigma^2$ is as follows (square root for standard deviation $\sigma$):

$$
\sigma^2_X = \frac{\sum (x_i - \bar{x})^2}{n}
$$

<br />

Covariance $\sigma_{X,Y}$ is as follows - we cannot interpret the actual value, only the sign:

$$
cov(X,Y) = \frac{\sum (x_i - \bar{x})(y_i - \bar{y})}{n} = E[ \space (x_i - E[X]) \times (y_i - E[Y]) \space ]
$$

Correlation coefficients $r, \rho_{X,Y}$ are a normalised form of covariance (always between -1 and 1), so we can interpret the strength of correlation:

$$
corr(X,Y)=r = \rho_{X,Y} = \frac{cov(X,Y)}{\sigma_X \sigma_Y}
$$

$r^2$ is the square of the correlation coefficient - tells us what percentage of the variation of $Y$ is explained by $X$

$$
r^2 = \left( \frac{cov(X,Y)}{\sigma_X \sigma_Y} \right )^2$$

<br />

Linear best fit lines take the following form:

$$
y = \alpha + \beta x
$$

The slope $\beta$ of the best fit line is related to covariance. Tells us the magnitude of the effect of $X$ on $Y$

$$
\beta_1 = \frac{cov(X,Y)}{\sigma^2_X}
$$

<br />

### What is a Correlation?

The correlation between two features, is the extent to which they tend to occur together

-   This tells us the relationship between two things

-   If these two things tend to occur together, it is a positive relationship

-   If the occurrence of one feature means the other tends not to occur, it is a negative relationship

-   If the occurrence of one feature has no effect on the occurrence of another, then there is no relationship

<br />

What does it mean for features to occur together?

-   Take two features $A$ and $B$, each with only two values $0,1$

    -   $A$ represents a "major oil producer". If a country produces more than 40,000 barrels per day per million people, then it gets a value of $1$. If not, it is $0$

    -   $B$ represents democracies ( $1$ ) vs. autocracies ( $0$ )

-   We can figure out if they are correlated, by making a comparison

    -   Are major oil producers more likely to be autocracies than countries who are not major oil producers?

    -   Are autocracies more likely to be major oil producers than democracies?

-   These questions of comparison help us determine if there is a correlation

<br />

Knowledge of correlation is useful:

-   If we know the direction of relationship (positive/negative), that could be useful for prediction

    -   For example, if we know the relationship between major oil producers and democracy/autocracy, we can better predict if a country will be a democracy given their oil production

-   If we know the direction of relationship (positive/negative), that could also be useful for causation

    -   After all, for something to cause something else, we would expect that changing one feature would affect the other (hence, a correlation)

<br />

For non-binary variables, we can use a scatterplot to plot correlations

-   Put one feature on the $x$ axis, and one feature on the $y$ axis

-   Often, just by looking at the scatterplot, we can tell the direction of the correlation

<br />

### Fact or Correlation?

In order to establish a correlation, you must always make a comparison of some kind

-   For example, if we want to learn about the correlation between temperature and crime, we need to **compare** hot and cold days, and how crime varies with them. We could also **compare** low and high crime days, and how temperature varies with them

-   However, if we don't make a comparison, it is just a fact

    -   For example, this is a fact: Hot days have high crime

-   To make this a correlation - we have to add a comparison

    -   When the day is more hot, there is more crime

<br />

Take these following examples:

1.  Fact: People who live to be 100 years old typically take vitamins
2.  Correlation: Cities with more crime tend to hire more police officers
3.  Fact: Successful people have spend at least 10,000 hours honing their craft
4.  Fact: Most politicians engulfed in a scandal win re-election
5.  Correlation: Older people vote more than younger people

<br />

### What is a Correlation Good For?

Correlation is good for description, forecasting, and causal inference

-   Description: this is the most straight forward - describing reality - the relationship between 2 features.

    -   This is often very important for identifying problems - observing reality

-   Forecasting/Prediction: If you know the relationship is positive, then you know increasing one variable will likely increase the other variable, even if we have no data on that new point

    -   Forecasting is important in prediction of elections, who will vote, predicting disasters, predicting length of recession, public policy, etc.

    -   However, we have to be careful - past correlations may not always hold i nthe future

-   Causal inference: you can't have causality without co-variation

    -   Since, by definition, causation means changing one thing will change the other

    -   Causation is very useful for a variety of topics in the social sciences - especially in designing better policy. It is also important with decision making

    -   If we know what causes something, we can then either do that thing that causes that thing (if the outcome is good), or avoid doing that thing that causes that thing (if the outcome is bad)

<br />

### Measuring Correlations

Before we start, let us get some notation out of the way:

-   A subscript $i$ refers to one specific observation. So, an observation of $x$ would be denoted as $x_i$

-   Mean is denoted with $\mu$, or with a bar over the variable $\bar{x}$, or with the function $E[X]$

-   Variance is detonated as $\sigma^2$, and standard deviation is denoted as $\sigma$

<br />

Simple descriptive statistics are as follows:

-   Mean/average, $\mu$: is simply the sum of all the observations $\sum u_i$ , divided by the number of observations $N$

-   Variance: $\sigma^2$: a measure of how far the mean tends to be from the individual values - a measure of spread

    -   Calculated by first, finding the distance between a point, and the mean of the variable

    -   Then squaring that distance

    -   Then doing the same for every point

    -   Now sum all the squared values you have, and divide by the number of observations

    -   A mathematical formula is given below

-   Standard deviation $\sigma$ is the square root of variance $\sigma^2$ - denotes how far we expect observations to be from the mean, on average

Variance, more mathematically, is defined as:

$$
\sigma^2_X = \frac{\sum\limits_i^N (x_i - \bar{x})^2}{N}
$$

<br />

Covariance is calculated as follows:

-   Lets say we have 2 variables, $X$ and $Y$, and thus, a set of pairs $(x_i, y_i)$

-   For an observation, take the value of the point $x_i$, and subtract the mean $\bar{x}$ from it. Do the same for $y$

-   Now, multiply the differences we just calculated

-   Now, do the above for all the ordered pairs $(x_i, y_i)$

-   Sum up all the products, then divide by the number of ordered pairs

More mathematically:

$$
cov(X,Y) = \frac{\sum\limits_i^N (x_i - \bar{x})(y_i - \bar{y})}{N} = E[ \space (x_i - E[X]) \times (y_i - E[Y]) \space ]
$$

We cannot interpret the magnitude of covariance, however, we can interpret the sign

-   Positive covariance is a positive relationship

-   Negative covariance is a negative relationship

-   0 covariance is no relationship

<br />

Correlation Coefficients are a way to interpret the strength of a correlation

-   It normalises covariance based on the variance - so that the correlation coefficient always sits between -1 and 1

-   This normalised scale allows us to compare across different sets of data, and interpret the strength of a correlation

Correlation coefficients are calculated simply by dividing the covariance by the standard deviation of $X$ and the standard deviation of $Y$. Or mathematically:

$$
corr(X,Y)=r = \frac{cov(X,Y)}{\sigma_X \sigma_Y}
$$

We often use $r$ to notate the correlation coefficient:

-   A value closer to +1 is a strong positive correlation

-   A value closer to -1 is a strong negative correlation

-   A value closer to 0 is a weak correlation

-   0 is when there is no correlation at all

<br />

We can square the correlation coefficient $r$ to get $r^2$

-   This is always between 0 and 1 (since the square gets rid of the negatives)

-   This allows us to see the "proportion of $Y$ explained by $X$"

-   This is particularly useful in multivariate regression models, to show how good our predictors $X_1, X_2, ..., X_n$are at prediction $Y$

<br />

Slope of the regression line (best fit linear line) is another way to measure the relationship between $X$ and $Y$

-   This value not only tells us how strong a relationship is, but the magnitude of the relationship

-   For example, you can have a perfectly strong relationship with almost perfect correlation. However, the actual effect of that relationship could be very small - one change in $X$ barely changes $Y$, but the correlation is still strong since the points are almost directly on top of the best fit line

-   We are often interested in the magnitude of the relationship - after all, who cares if there is a correlation, but it doesn't mean much

The slope of the regression best fit line is the following formula:

$$
\beta_1 = \frac{cov(X,Y)}{\sigma^2_X}
$$

<br />

### Straight Talk about Linearity

So far, we have focused on linear relationships

-   But, not all relationships are linear - they can be quadratic, exponential, logarithmic, etc.

-   These non-linear relationships often don't appear on our statistics of covariance and correlation, however, we can see them on a scatterplot

-   We will explore approaches dealing with non-linearity later, such as transforming variables so they are more linear, or non-linear regression methods

    -   For example, instead of finding the correlation between $X$ and $Y$, we could look at the correlation between $X^2$ and $Y$

-   We focus on linear relationships, because even non-linear relationships mirror linearity when we "zoom in".

    -   However, we have to be careful about problems of extrapolation with these scenarios.

<br />

------------------------------------------------------------------------

[GV481 Homepage](https://kevinli03.github.io/notes/#GV481_Quantitative_Analysis)
